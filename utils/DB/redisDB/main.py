from datetime import datetime, timedelta
import numpy as np
import pandas as pd
import redis, json, time
from tzlocal import get_localzone  # ì‹œìŠ¤í…œ ë¡œì»¬ íƒ€ì„ì¡´ ìë™ ê°ì§€
import json
# aioredis í˜¸í™˜ ì„í¬íŠ¸ (ì—†ìœ¼ë©´ redis.asyncio ì‚¬ìš©)
try:
    import aioredis  # type: ignore
    from aioredis import RedisError as RedisError  # type: ignore
except Exception:  # pragma: no cover
    from redis import asyncio as aioredis  # type: ignore
    from redis.exceptions import RedisError  # type: ignore
# ì‹œìŠ¤í…œì˜ ë¡œì»¬ íƒ€ì„ì¡´ ê°€ì ¸ì˜¤ê¸°
local_tz = get_localzone()

class RedisManager:
    """
    Redis ê¸°ë³¸ ë°ì´í„°, í•´ì‹œ ë°ì´í„°, ì‹œê³„ì—´ ë°ì´í„° ë° ë°±ì—…ì„ í†µí•© ê´€ë¦¬í•˜ëŠ” ì»¨í…ìŠ¤íŠ¸
    """
    def __init__(self, host='localhost', port=6379, db=0, max_connections=20, password=None):
        """
        Redis ì—°ê²° ì´ˆê¸°í™” (ë¹„ë°€ë²ˆí˜¸ ì¸ì¦ í¬í•¨)
        
        :param host: Redis ì„œë²„ í˜¸ìŠ¤íŠ¸
        :param port: Redis ì„œë²„ í¬íŠ¸
        :param db: Redis ë°ì´í„°ë² ì´ìŠ¤ ë²ˆí˜¸
        :param max_connections: ìµœëŒ€ ì—°ê²° ìˆ˜
        :param password: Redis ë¹„ë°€ë²ˆí˜¸ (ì—†ìœ¼ë©´ None)
        """
        self.host = host
        self.port = port
        self.db = db
        self.max_connections = max_connections
        self.password = password        
        self.lua_script = None

    def connect(self):
        """
        Redis ì„œë²„ì— ì—°ê²° (ì¬ì—°ê²° í¬í•¨)
        """
        try:
            pool = redis.ConnectionPool(
                host=self.host, port=self.port, db=self.db,
                decode_responses=True, max_connections=self.max_connections,
                socket_timeout=10,  # 10ì´ˆ í›„ íƒ€ì„ì•„ì›ƒ
                socket_connect_timeout=10,  # ì—°ê²° íƒ€ì„ì•„ì›ƒ 10ì´ˆ
                retry_on_timeout=True,  # íƒ€ì„ì•„ì›ƒ ì‹œ ì¬ì‹œë„
            )
            self.client = redis.Redis(connection_pool=pool)
            if self.password:
                self.client.config_set("requirepass", self.password)

        except redis.AuthenticationError:
            pool = redis.ConnectionPool(
                host=self.host, port=self.port, db=self.db,
                password=self.password, decode_responses=True,
                max_connections=self.max_connections,
                socket_timeout=10,  # 10ì´ˆ í›„ íƒ€ì„ì•„ì›ƒ
                socket_connect_timeout=10,  # ì—°ê²° íƒ€ì„ì•„ì›ƒ 10ì´ˆ
                retry_on_timeout=True,  # íƒ€ì„ì•„ì›ƒ ì‹œ ì¬ì‹œë„
            )
            self.client = redis.Redis(connection_pool=pool)

    def is_connected(self):
        """ì—°ê²° ìƒíƒœ í™•ì¸ (ping í…ŒìŠ¤íŠ¸)"""
        try:
            return self.client.ping() if self.client else False
        except redis.ConnectionError:
            return False
        
    # ------------------------------
    # ğŸ“Œ ì¼ë°˜ ë°ì´í„° ì €ì¥ ë° ì¡°íšŒ
    # ------------------------------

    def set_value(self, key, value, expire=None):
        """ì¼ë°˜ ë°ì´í„° ì €ì¥"""
        value = json.dumps(value)
        self.client.set(key, value, ex=expire)

    def get_value(self, key):
        """ì¼ë°˜ ë°ì´í„° ì¡°íšŒ"""
        value = self.client.get(key)
        return json.loads(value) if value else None

    def delete_value(self, key):
        """íŠ¹ì • í‚¤ ì‚­ì œ"""
        self.client.delete(key)

    def exists(self, key):
        """í‚¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        return self.client.exists(key) > 0

    def flush(self):
        """ëª¨ë“  ë°ì´í„° ì‚­ì œ"""
        self.client.flushdb()

    def get_all_keys(self):
        """ëª¨ë“  í‚¤ ëª©ë¡ ì¡°íšŒ"""
        return self.client.keys('*')

    def mget(self, keys, as_dict=True):
        """ì—¬ëŸ¬ í‚¤ë¥¼ í•œ ë²ˆì— ì¡°íšŒ(MGET)í•˜ì—¬ JSON ë””ì½”ë”©í•´ ë°˜í™˜
        :param keys: ì¡°íšŒí•  í‚¤ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ë‹¨ì¼ í‚¤
        :param as_dict: Trueë©´ {key: value} dict, Falseë©´ ê°’ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        """
        if isinstance(keys, (str, bytes)):
            keys = [keys]
        values = self.client.mget(keys)
        decoded = []
        for v in values:
            try:
                decoded.append(json.loads(v) if v else None)
            except Exception:
                decoded.append(v)
        return {k: v for k, v in zip(keys, decoded)} if as_dict else decoded

    # ------------------------------
    # ğŸ“Œ Bulk ë°ì´í„° ì²˜ë¦¬ (bulk_create, bulk_update)
    # ------------------------------

    def bulk_set(self, data, expire=None):
        """
        ì—¬ëŸ¬ ê°œì˜ í‚¤-ê°’ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì €ì¥ (bulk_create ì—­í• )
        :param data: {key1: value1, key2: value2, ...} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
        :param expire: ë§Œë£Œ ì‹œê°„ (ì´ˆ) (ì„ íƒ ì‚¬í•­)
        
        ğŸ“Œ ì‚¬ìš© ì˜ˆì‹œ:
        redis_manager.bulk_set({
            "user:1001": {"name": "Alice", "age": 30},
            "user:1002": {"name": "Bob", "age": 25}
        })
        """
        pipeline = self.client.pipeline()
        for key, value in data.items():
            pipeline.set(key, json.dumps(value), ex=expire)
        pipeline.execute()

    def bulk_update(self, data, expire=None):
        """
        ì—¬ëŸ¬ ê°œì˜ í‚¤-ê°’ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì—…ë°ì´íŠ¸ (bulk_update ì—­í• )
        :param data: {key1: value1, key2: value2, ...} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
        :param expire: ë§Œë£Œ ì‹œê°„ (ì´ˆ) (ì„ íƒ ì‚¬í•­)

        ğŸ“Œ ì‚¬ìš© ì˜ˆì‹œ:
        redis_manager.bulk_update({
            "user:1001": {"age": 31},  # ê¸°ì¡´ í‚¤ ì—…ë°ì´íŠ¸
            "user:1002": {"city": "Seoul"}  # ìƒˆë¡œìš´ í•„ë“œ ì¶”ê°€
        })
        """
        pipeline = self.client.pipeline()
        for key, new_value in data.items():
            existing_value = self.client.get(key)
            if existing_value:
                updated_value = json.loads(existing_value)
                if isinstance(updated_value, dict) and isinstance(new_value, dict):
                    updated_value.update(new_value)
                else:
                    updated_value = new_value
            else:
                updated_value = new_value
            pipeline.set(key, json.dumps(updated_value), ex=expire)
        pipeline.execute()

    # ------------------------------
    # ğŸ“Œ í•´ì‹œ ë°ì´í„° Bulk ì €ì¥ ë° ì—…ë°ì´íŠ¸ (ë™ê¸°)
    # ------------------------------

    def hbulk_set(self, name, data):
        """
        ì—¬ëŸ¬ ê°œì˜ í•´ì‹œ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì €ì¥ (bulk_create ì—­í• )
        :param name: Redis í•´ì‹œ í‚¤ ì´ë¦„
        :param data: {field1: value1, field2: value2, ...} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
        
        ğŸ“Œ ì‚¬ìš© ì˜ˆì‹œ:
        redis_manager.hbulk_set("user:1001", {
            "email": "alice@example.com",
            "phone": "123-456-7890"
        })
        """
        pipeline = self.client.pipeline()
        for field, value in data.items():
            pipeline.hset(name, field, json.dumps(value))
        pipeline.execute()

    def hbulk_update(self, name, data):
        """
        ì—¬ëŸ¬ ê°œì˜ í•´ì‹œ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì—…ë°ì´íŠ¸ (bulk_update ì—­í• )
        :param name: Redis í•´ì‹œ í‚¤ ì´ë¦„
        :param data: {field1: value1, field2: value2, ...} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
        
        ğŸ“Œ ì‚¬ìš© ì˜ˆì‹œ:
        redis_manager.hbulk_update("user:1001", {
            "phone": "987-654-3210",  # ê¸°ì¡´ í•„ë“œ ì—…ë°ì´íŠ¸
            "address": "New York"  # ìƒˆë¡œìš´ í•„ë“œ ì¶”ê°€
        })
        """
        pipeline = self.client.pipeline()
        for field, new_value in data.items():
            existing_value = self.client.hget(name, field)
            if existing_value:
                updated_value = json.loads(existing_value)
                if isinstance(updated_value, dict) and isinstance(new_value, dict):
                    updated_value.update(new_value)
                else:
                    updated_value = new_value
            else:
                updated_value = new_value
            pipeline.hset(name, field, json.dumps(updated_value))
        pipeline.execute()
    # ------------------------------
    # ğŸ“Œ í•´ì‹œ ë°ì´í„° ì €ì¥ ë° ì¡°íšŒ
    # ------------------------------

    def hset(self, name, key, value):
        """í•´ì‹œ ë°ì´í„° ì €ì¥"""
        value = json.dumps(value)
        self.client.hset(name, key, value)

    def hget(self, name, key):
        """í•´ì‹œ ë°ì´í„° ì¡°íšŒ"""
        value = self.client.hget(name, key)
        return json.loads(value) if value else None

    def hmset(self, name, mapping):
        """í•´ì‹œ ë°ì´í„° ì—¬ëŸ¬ ê°œ ì €ì¥"""
        # ëª¨ë“  ê°’ì„ JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        mapping = {key: json.dumps(value) for key, value in mapping.items()}
        self.client.hmset(name, mapping)
        
    def hmget(self, name, keys):
        """í•´ì‹œ ë°ì´í„° ì—¬ëŸ¬ ê°œ ì¡°íšŒ"""
        values = self.client.hmget(name, keys)
        
        # JSON ë””ì½”ë”© (ê°’ì´ ì¡´ì¬í•  ê²½ìš°)
        return {key: json.loads(value) if value else None for key, value in zip(keys, values)}
    
    def hexists(self, name, key):
        """í‚¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        return self.client.hexists(name, key) > 0

    def hcreate_or_update(self, name, data):
        """í•´ì‹œ ë°ì´í„° ì—…ë°ì´íŠ¸ ë˜ëŠ” ìƒì„±"""
        for key, value in data.items():
            existing_value = self.client.hget(name, key)

            if existing_value:
                updated_value = json.loads(existing_value)
                if isinstance(updated_value, dict) and isinstance(value, dict):
                    updated_value.update(value)
                else:
                    updated_value = value
            else:
                updated_value = value

            self.client.hset(name, key, json.dumps(updated_value))

    # ------------------------------
    # ğŸ“Œ ì¼ë°˜ í‚¤ ì—…ë°ì´íŠ¸ (ì—†ìœ¼ë©´ ìƒì„±)
    # ------------------------------

    def create_or_update(self, key, value, expire=None):
        """ì¼ë°˜ í‚¤ ë°ì´í„° ì—…ë°ì´íŠ¸ ë˜ëŠ” ìƒì„±"""
        existing_value = self.client.get(key)

        if existing_value:
            updated_value = json.loads(existing_value)
            updated_value.update(value)
        else:
            updated_value = value

        updated_value = json.dumps(updated_value)
        self.client.set(key, updated_value, ex=expire)

    # ------------------------------
    # ğŸ“Œ ì‹œê³„ì—´ ë°ì´í„° ê´€ë¦¬ ê¸°ëŠ¥ (TimeSeries)
    # ------------------------------

    def create_timeseries(self, key, retention=0, labels=None):
        """
        ì‹œê³„ì—´ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± (í•œ ë²ˆì— ë¼ë²¨ ì¶”ê°€)
        
        ì‚¬ìš© ì˜ˆì‹œ:
        tsdb.create_timeseries("sensor:1", retention=60000, labels={"location": "kitchen", "type": "temperature"})
        """
        if labels is None:
            labels = {}

        
        # 1ï¸âƒ£ í‚¤ê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        exists = self.client.exists(key)

        if exists:
            # 2ï¸âƒ£ ì´ë¯¸ ì¡´ì¬í•˜ë©´ TS.ALTER ì‚¬ìš©í•˜ì—¬ ì„¤ì • ì—…ë°ì´íŠ¸
            label_args = []
            for label, value in labels.items():
                label_args.extend([label, value])

            self.client.execute_command('TS.ALTER', key, 'DUPLICATE_POLICY', 'LAST', 'RETENTION', retention, 'LABELS', *label_args)
        else:
            # 3ï¸âƒ£ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒˆë¡­ê²Œ ìƒì„±
            label_args = []
            for label, value in labels.items():
                label_args.extend([label, value])
            self.client.execute_command('TS.CREATE', key, 'DUPLICATE_POLICY', 'FIRST', 'RETENTION', retention, 'LABELS', *label_args)

    def add_timeseries_data(self, key, value, timestamp=None):
        """
        ì‹œê³„ì—´ ë°ì´í„° ì¶”ê°€
        :param callback ['get_labels_callback']
        
        ì‚¬ìš© ì˜ˆì‹œ:
        tsdb.add_timeseries_data("sensor:1", 22.5)
        """
        if timestamp is None:
            timestamp = int(time.time() * 1000)
        self.client.execute_command('TS.ADD', key, timestamp, value)

    def get_timeseries_range(self, key_pattern, start='-', end='+', count=None, callback=None):
        """
        íŠ¹ì • ê¸°ê°„ì˜ ì‹œê³„ì—´ ë°ì´í„° ì¡°íšŒ (ì™€ì¼ë“œì¹´ë“œ ì§€ì›) + COUNT ì˜µì…˜ ì¶”ê°€ 
        :param key_pattern: ê²€ìƒ‰í•  í‚¤ íŒ¨í„´ (ì˜ˆ: "alert:*")
        :param start: ì¡°íšŒ ì‹œì‘ ì‹œê°„ ('-'ëŠ” ê°€ì¥ ì˜¤ë˜ëœ ë°ì´í„°)
        :param end: ì¡°íšŒ ì¢…ë£Œ ì‹œê°„ ('+'ëŠ” ìµœì‹  ë°ì´í„°)
        :param count: ì¡°íšŒí•  ë°ì´í„° ê°œìˆ˜ ì œí•œ (Optional)
        :param callback: í›„ì²˜ë¦¬ ì½œë°± í•¨ìˆ˜ (Optional)['get_labels_callback']
        ì‚¬ìš© ì˜ˆì‹œ:
        tsdb.get_timeseries_range("alert:*", start=1672531200000, end=1672617600000, count=100)
        """
        try:
            keys = []
            
            # ì™€ì¼ë“œì¹´ë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸
            if '*' in key_pattern:
                # SCAN ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¼ì¹˜í•˜ëŠ” í‚¤ ì°¾ê¸°
                cursor = 0
                while True:
                    cursor, found_keys = self.client.scan(cursor, match=key_pattern, count=100)
                    keys.extend(found_keys)
                    if cursor == 0:
                        break
            else:
                keys.append(key_pattern)  # ë‹¨ì¼ í‚¤ ì¡°íšŒ

            if not keys:
                return {"error": "No matching TimeSeries keys found."}

            # Redis Pipeline ì‚¬ìš©
            pipeline = self.client.pipeline()
            for key in keys:
                if count:
                    pipeline.execute_command("TS.RANGE", key, start, end, "COUNT", count)
                else:
                    pipeline.execute_command("TS.RANGE", key, start, end)
            
            # ê²°ê³¼ ì‹¤í–‰
            results = pipeline.execute()
            # ë°ì´í„° ì •ë¦¬
            data = {
                key: [{"timestamp": ts, "value": value} for ts, value in result]
                for key, result in zip(keys, results)
            }
            # ì½œë°±ì´ ì œê³µëœ ê²½ìš° ì‹¤í–‰
            if callback:
                return callback(data, [key])
            else:
                return data

        except redis.exceptions.RedisError as e:
            return {"error": f"Redis error: {str(e)}"}
        
    def get_labels_callback(self, result, keys):
        """
        í´ë˜ìŠ¤ ë‚´ë¶€ì˜ ì½œë°± í•¨ìˆ˜ (ë°ì´í„°ë¥¼ ì¶”ê°€ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ)
        """
        try:
            if not keys:
                return result

            # Redis Pipeline ì‚¬ìš©
            pipeline = self.client.pipeline()
            for key in keys:
                pipeline.execute_command("TS.INFO", key)
            
            info_results = pipeline.execute()

            # ë¼ë²¨ ë°ì´í„° ì •ë¦¬
            labels_dict = {}
            for key, info in zip(keys, info_results):
                try:
                    # info ë¦¬ìŠ¤íŠ¸ì—ì„œ 'labels' í‚¤ ì°¾ê¸°
                    label_index = info.index("labels") if "labels" in info else -1
                    if label_index != -1 and label_index + 1 < len(info):
                        raw_labels = info[label_index + 1]  # ë¼ë²¨ ë¦¬ìŠ¤íŠ¸
                        if isinstance(raw_labels, list):
                            # ë¦¬ìŠ¤íŠ¸ ë‚´ ë¦¬ìŠ¤íŠ¸ êµ¬ì¡°ë¥¼ {key: value} í˜•íƒœë¡œ ë³€í™˜
                            labels_dict[key] = {str(entry[0]): str(entry[1]) for entry in raw_labels if isinstance(entry, list) and len(entry) == 2}
                        else:
                            labels_dict[key] = {}
                    else:
                        labels_dict[key] = {}

                except ValueError:
                    labels_dict[key] = {}  # ì˜ˆì™¸ ë°œìƒ ì‹œ ë¹ˆ ë¼ë²¨ ì €ì¥

            return result, labels_dict  # ë¼ë²¨ ì •ë³´ í¬í•¨í•˜ì—¬ ë°˜í™˜

        except redis.exceptions.RedisError as e:
            return result

    def timeseries_exists(self, key):
        """
        Redis Time Series ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜

        ì‚¬ìš© ì˜ˆì‹œ:
        exists = tsdb.timeseries_exists("sensor:1")
        print(exists)  # True ë˜ëŠ” False ë°˜í™˜
        """
        try:
            # TS.INFOë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ì˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            totalSamples = self.client.execute_command('TS.INFO', key)
            if totalSamples[1] > 0:
                return True  # ë°ì´í„°ê°€ ì¡´ì¬í•˜ë©´ True ë°˜í™˜
            else:
                return False  # ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ False ë°˜í™˜
        except Exception as e:
            if "TSDB: the key does not exist" in str(e):
                return False  # ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ False ë°˜í™˜
            else:
                raise  # ê¸°íƒ€ ì˜¤ë¥˜ëŠ” ê·¸ëŒ€ë¡œ ë°œìƒ


    def get_latest_timeseries(self, key, callback=None):
        """
        ìµœì‹  ì‹œê³„ì—´ ë°ì´í„° ì¡°íšŒ + ë¼ë²¨ ì •ë³´ í¬í•¨ (ì˜µì…˜)
        :param callback ['get_labels_callback']
        
        ì‚¬ìš© ì˜ˆì‹œ:
        latest = tsdb.get_latest_timeseries("sensor:1")
        print(latest)
        """
        try:
            # ìµœì‹  ë°ì´í„° ì¡°íšŒ
            result = self.client.execute_command('TS.GET', key)
            if not result:
                return ModuleNotFoundError
            # ì½œë°±ì´ ì œê³µëœ ê²½ìš° ì‹¤í–‰
            if callback:
                return callback(result, [key])
            else:
                return result

        except redis.exceptions.RedisError as e:
            return {"error": f"Redis error: {str(e)}"}

    def get_pattern_latest_timeseries(self, key_pattern, callback=None):
        """
        ì™€ì¼ë“œì¹´ë“œ íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ì‹œê³„ì—´ í‚¤ì˜ ìµœì‹  ë°ì´í„° ì¡°íšŒ
        :param callback ['get_labels_callback']

        ì‚¬ìš© ì˜ˆì‹œ:
        latest = tsdb.get_pattern_latest_timeseries("sensor:*")
        print(latest)
        """
        try:
            # íŒ¨í„´ì— '*'ê°€ ì—†ìœ¼ë©´ ì˜ˆì™¸ ì²˜ë¦¬
            if '*' not in key_pattern:
                return {"error": "This function only supports wildcard patterns. Use get_latest_timeseries for single keys."}

            keys = []
            cursor = 0

            # SCANì„ ì‚¬ìš©í•˜ì—¬ íŒ¨í„´ì— ë§ëŠ” í‚¤ ê²€ìƒ‰
            while True:
                cursor, found_keys = self.client.scan(cursor, match=key_pattern, count=100)
                keys.extend(found_keys)
                if cursor == 0:
                    break

            if not keys:
                return {"error": "No matching TimeSeries keys found."}

            # Redis Pipeline ì‚¬ìš©
            pipeline = self.client.pipeline()
            for key in keys:
                pipeline.execute_command("TS.GET", key)
            
            # ê²°ê³¼ ì‹¤í–‰
            results = pipeline.execute()

            # ë°ì´í„° ì •ë¦¬
            data = {
                key: {"timestamp": result[0], "value": result[1]} if result else None
                for key, result in zip(keys, results)
            }

            # ì½œë°±ì´ ì œê³µëœ ê²½ìš° ì‹¤í–‰
            if callback:
                return callback(data, keys)
            else:
                return data

        except redis.exceptions.RedisError as e:
            return {"error": f"Redis error: {str(e)}"}

    def delete_timeseries(self, key):
        """
        ì‹œê³„ì—´ í‚¤ ì‚­ì œ
        
        ì‚¬ìš© ì˜ˆì‹œ:
        tsdb.delete_timeseries("sensor:1")
        """
        self.client.delete(key)

    def query_scan(self, pattern):
        """
        Redisì˜ ëª¨ë“  í‚¤ë¥¼ ì¡°íšŒ (SCAN ì‚¬ìš©)
        :param client: Redis í´ë¼ì´ì–¸íŠ¸
        :return: ëª¨ë“  í‚¤ ë¦¬ìŠ¤íŠ¸
        """
        cursor = 0
        matched_keys = []

        # 1ï¸âƒ£ SCANì„ ì´ìš©í•´ ë°˜ë³µ ê²€ìƒ‰
        while True:
            cursor, keys = self.client.execute_command("SCAN", cursor, "MATCH", pattern, "COUNT", 100)
            
            # 2ï¸âƒ£ keysê°€ ì¡´ì¬í•  ë•Œë§Œ ì¶”ê°€
            if keys and len(keys) > 0:
                matched_keys.extend(keys)

            # 3ï¸âƒ£ ì»¤ì„œê°€ 0ì´ë©´ ì¢…ë£Œ
            if cursor == 0:
                break

        return matched_keys
    
    def query_keys(self, pattern):
        """
        íŒ¨í„´ì„ ê¸°ë°˜ìœ¼ë¡œ íŠ¹ì • í‚¤ë§Œ ì¡°íšŒ
        :param pattern: ["*", "9", "Sensor", "*", "*"] ê°™ì€ ë¦¬ìŠ¤íŠ¸ í˜•ì‹
        """
        # 1ï¸âƒ£ ëª¨ë“  í‚¤ ê°€ì ¸ì˜¤ê¸°
        keys = self.client.keys('*')

        # 2ï¸âƒ£ "*"ì„ ì œì™¸í•œ í•„í„°ë§í•  ê°’ë§Œ ì¶”ì¶œ
        filters = [p for p in pattern if p != "*"]

        # 3ï¸âƒ£ í•„í„°ë§ ì ìš© (ëª¨ë“  í‚¤ì—ì„œ í•„í„°ë§í•  ê°’ì´ ìˆëŠ”ì§€ í™•ì¸)
        result_keys = []
        for key in keys:
            if all(f in key for f in filters):
                result_keys.append(key)

        return result_keys
    
    def query_by_label(self, label_filter):
        """
        ë¼ë²¨ì„ ê¸°ë°˜ìœ¼ë¡œ í•´ë‹¹í•˜ëŠ” ëª¨ë“  ì‹œê³„ì—´ í‚¤ ì¡°íšŒ
        
        ì‚¬ìš© ì˜ˆì‹œ:
        keys = tsdb.query_by_label("location=kitchen")
        print(keys)  # ['sensor:1', 'sensor:3']
        """
        return self.client.execute_command('TS.QUERYINDEX', label_filter)

    def get_data_by_label(self, label_filter, start='-', end='+'):
        """
        ë¼ë²¨ì„ ê¸°ë°˜ìœ¼ë¡œ ì—¬ëŸ¬ ì‹œê³„ì—´ ë°ì´í„° ì¡°íšŒ
        
        ì‚¬ìš© ì˜ˆì‹œ:
        data = tsdb.get_data_by_label("location=kitchen")
        print(data)
        """
        return self.client.execute_command('TS.MRANGE', start, end, 'FILTER', label_filter)
    

    def custom_aggregate(self, source_keys, query_key='*', aggregate_type=['MIN', 'MAX', 'AVG'], batch_size=10, minute=5, delay=0):
        """
        source_keys: ì§‘ê³„í•  ë°ì´í„° í‚¤ ë¦¬ìŠ¤íŠ¸
        aggregate_type: ì ìš©í•  ì§‘ê³„ ì—°ì‚° ë¦¬ìŠ¤íŠ¸
        batch_size: Pipelineì„ í†µí•´ í•œ ë²ˆì— ì²˜ë¦¬í•  í‚¤ ê°œìˆ˜
        delay: ìŠ¤ì¼€ì¤„ëŸ¬ ë™ì‘ ì§€ì—°ì‹œê°„ 
        ["MIN", "MAX", "AVG", "SUM", "COUNT", "FIRST", "LAST", "STDDEV",
        "VARIANCE", "RANGE", "DIFFERENCE", "DERIVATIVE", "NONNEGATIVE_DERIVATIVE",
        "HISTOGRAM", "SPREAD", "MEDIAN", "SKEW", "CHANGE_TIME", "RUN_TIME", "DOWN_TIME", "UP_PULSE_TIME", "DOWN_PULSE_TIME"]
        - MIN: ìµœì†Œê°’ ê³„ì‚°
        - MAX: ìµœëŒ€ê°’ ê³„ì‚°
        - AVG: í‰ê· ê°’ ê³„ì‚°
        - SUM: í•©ê³„ ê³„ì‚°
        - COUNT: ë°ì´í„° ê°œìˆ˜ ê³„ì‚°
        - FIRST: ì²« ë²ˆì§¸ ê°’
        - LAST: ë§ˆì§€ë§‰ ê°’
        - STDDEV: í‘œì¤€í¸ì°¨ ê³„ì‚°
        - VARIANCE: ë¶„ì‚° ê³„ì‚°
        - RANGE: ê°’ì˜ ë²”ìœ„ (ìµœëŒ€ - ìµœì†Œ)
        - DIFFERENCE: ê° ê°’ ê°„ ì°¨ì´ ê³„ì‚°
        - DIFFERENCE_SUM: ì´ ë³€í™”ëŸ‰ ê³„ì‚°
        - DERIVATIVE: ë³€í™”ìœ¨ ê³„ì‚°
        - NONNEGATIVE_DERIVATIVE: ìŒìˆ˜ê°€ ì—†ëŠ” ë³€í™”ìœ¨ ê³„ì‚°
        - HISTOGRAM: íˆìŠ¤í† ê·¸ë¨ (ë¹ˆë„ìˆ˜ ë¶„í¬)
        - SPREAD: ë°ì´í„°ì˜ ìµœëŒ€ê°’ê³¼ ìµœì†Œê°’ ì°¨ì´
        - MEDIAN: ì¤‘ì•™ê°’ ê³„ì‚°
        - SKEW: ë°ì´í„°ì˜ ì™œë„ ê³„ì‚°
        - CHANGE_TIME: ë§ˆì§€ë§‰ìœ¼ë¡œ ê°’ì´ ë³€í•œ ì‹œì 
        - RUN_TIME: ê°’ì´ 0ì´ ì•„ë‹Œ ì‹œê°„ì˜ ì´í•© (ë‹¨ìœ„: ì´ˆ)
        - DOWN_TIME: ê°’ì´ 0ì¸ ì‹œê°„ì˜ ì´í•© (ë‹¨ìœ„: ì´ˆ)
        - UP_PULSE_TIME: ê°’ì´ 0ì—ì„œ 1 ì´ìƒìœ¼ë¡œ ë³€í•œ ì‹œì 
        - DOWN_PULSE_TIME: ê°’ì´ 1 ì´ìƒì—ì„œ 0ìœ¼ë¡œ ë³€í•œ ì‹œì 
        """
        results = {}
        ndigit = 4
        end_time = int(datetime.now().timestamp() * 1000) - delay * 1000 
        start_time = end_time - (minute * 60 * 1000)  # ìµœê·¼ 5ë¶„ ë°ì´í„°  
        total_time = (end_time - start_time) / 1000  # ì´ ì‹œê°„(ì´ˆ)
        
        for i in range(0, len(source_keys), batch_size):
            batch_keys = [key for key in source_keys[i:i + batch_size] if query_key in key or query_key == '*']
            
            with self.client.pipeline() as pipe:
                for source_key in batch_keys:
                    pipe.execute_command('TS.RANGE', source_key, start_time, end_time)
                responses = pipe.execute()
                
            for source_key, data in zip(batch_keys, responses):
                if not data:
                    results[source_key] = {agg: None for agg in aggregate_type}
                    continue

                df = pd.DataFrame(data, columns=['timestamp', 'value'])
                df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
                df.set_index('timestamp', inplace=True)
                
                # ê°’ì´ ë¬¸ìì—´ë¡œ ë°˜í™˜ë  ê²½ìš° ìˆ«ìë¡œ ë³€í™˜
                df['value'] = pd.to_numeric(df['value'], errors='coerce')
                df.dropna(subset=['value'], inplace=True)  # ë³€í™˜ë˜ì§€ ì•Šì€ ê°’ ì œê±°
                
                if df.empty:
                    results[source_key] = {agg: None for agg in aggregate_type}
                    continue
                
                # ë°ì´í„° íƒ€ì… í™•ì¸
                dtype = df['value'].dtype

                # int, float, bool ì´ì™¸ì˜ ê²½ìš° (ì˜ˆ: ë¬¸ìì—´) ì˜ˆì™¸ ì²˜ë¦¬
                if not np.issubdtype(dtype, np.number):
                    raise ValueError("value ì»¬ëŸ¼ì´ ìˆ«ìí˜•ì´ ì•„ë‹™ë‹ˆë‹¤.")
                
                agg_results = {}
                for agg in aggregate_type:
                    if agg == 'AVG':  # í‰ê· ê°’ ê³„ì‚°
                        agg_results['AVG'] = round(float(df['value'].mean()), ndigit) if np.issubdtype(dtype, np.floating) else int(df['value'].mean())
                    elif agg == 'SUM':  # í•©ê³„ ê³„ì‚°
                        agg_results['SUM'] = round(float(df['value'].sum()), ndigit) if np.issubdtype(dtype, np.floating) else int(df['value'].sum())
                    elif agg == 'MIN':  # ìµœì†Œê°’ ê³„ì‚°
                        agg_results['MIN'] = round(float(df['value'].min()), ndigit) if np.issubdtype(dtype, np.floating) else int(df['value'].min())
                    elif agg == 'MAX':  # ìµœëŒ€ê°’ ê³„ì‚°
                        agg_results['MAX'] = round(float(df['value'].max()), ndigit) if np.issubdtype(dtype, np.floating) else int(df['value'].max())
                    elif agg == 'COUNT':  # ë°ì´í„° ê°œìˆ˜ ê³„ì‚°
                        agg_results['COUNT'] = df['value'].count()
                    elif agg == 'FIRST':  # ì²« ë²ˆì§¸ ê°’
                        agg_results['FIRST'] = round(float(df['value'].iloc[0]), ndigit) if np.issubdtype(dtype, np.floating) else int(df['value'].iloc[0])
                    elif agg == 'LAST':  # ë§ˆì§€ë§‰ ê°’
                        agg_results['LAST'] = round(float(df['value'].iloc[-1]), ndigit) if np.issubdtype(dtype, np.floating) else int(df['value'].iloc[-1])
                    elif agg == 'STDDEV':
                        agg_results['STDDEV'] = round(float(df['value'].std()), ndigit)  # ğŸ”¹ float ë³€í™˜
                    elif agg == 'VARIANCE':
                        agg_results['VARIANCE'] = round(float(df['value'].var()), ndigit)  # ğŸ”¹ float ë³€í™˜
                    elif agg == 'RANGE':
                        agg_results['RANGE'] = round(float(df['value'].max() - df['value'].min()), ndigit) if np.issubdtype(dtype, np.floating) else int(df['value'].max() - df['value'].min())
                    elif agg == 'DIFFERENCE':
                        agg_results['DIFFERENCE'] = [round(float(x), ndigit) for x in df['value'].diff().dropna().tolist()]  # ğŸ”¹ ë¦¬ìŠ¤íŠ¸ ë‚´ ê°’ float ë³€í™˜
                    elif agg == 'DIFFERENCE_SUM':
                        diff_sum = df['value'].diff().dropna().sum()
                        agg_results['DIFFERENCE_SUM'] = round(diff_sum, ndigit) if diff_sum is not None else 0  # ğŸ”¹ float ë³€í™˜
                    elif agg == 'DERIVATIVE':
                        diff_mean = df['value'].diff().dropna().mean()
                        # NaNì´ ë‚˜ì˜¤ì§€ ì•Šë„ë¡ ì²´í¬ í›„ ë³€í™˜
                        agg_results['DERIVATIVE'] = round(float(diff_mean), ndigit) if not np.isnan(diff_mean) else 0
                    elif agg == 'NONNEGATIVE_DERIVATIVE':
                        agg_results['NONNEGATIVE_DERIVATIVE'] = round(float(df['value'].diff().clip(lower=0).dropna().mean()), ndigit)  # ğŸ”¹ float ë³€í™˜
                    elif agg == 'HISTOGRAM':
                        agg_results['HISTOGRAM'] = {k: int(v) for k, v in df['value'].value_counts().to_dict().items()}  # ğŸ”¹ int ë³€í™˜
                    elif agg == 'SPREAD':
                        agg_results['SPREAD'] = round(float(df['value'].max() - df['value'].min()), ndigit)  # ğŸ”¹ float ë³€í™˜
                    elif agg == 'MEDIAN':
                        agg_results['MEDIAN'] = round(float(df['value'].median()), ndigit)  # ğŸ”¹ float ë³€í™˜
                    elif agg == 'SKEW':
                        agg_results['SKEW'] = round(float(df['value'].skew()), ndigit)  # ğŸ”¹ float ë³€í™˜
                    elif agg == 'CHANGE_TIME':  # ë§ˆì§€ë§‰ìœ¼ë¡œ ê°’ì´ ë³€í•œ ì‹œì 
                        last_change = df[df['value'] != df['value'].shift()].index[-1]
                        # UTC íƒ€ì„ì¡´ ì§€ì • í›„, ë¡œì»¬ íƒ€ì„ì¡´ ë³€í™˜
                        last_change = last_change.tz_localize('UTC').tz_convert(local_tz)
                        agg_results['CHANGE_TIME'] = last_change
                    elif agg == 'RUN_TIME':  # ê°’ì´ 0ì´ ì•„ë‹Œ ì‹œê°„ì˜ ì´í•©
                        run_time_series = df[df['value'] > 0].index.to_series().diff().dropna().dt.total_seconds()
                        run_time = run_time_series.sum() if not run_time_series.empty else 0
                        agg_results['RUN_TIME'] = run_time
                    elif agg == 'DOWN_TIME':  # ê°’ì´ 0ì¸ ì‹œê°„ì˜ ì´í•©
                        down_time = total_time - agg_results.get('RUN_TIME', 0)
                        agg_results['DOWN_TIME'] = down_time
                    elif agg == 'UP_PULSE_TIME':
                        up_time = df[df['value'].diff() > 0].index.max()
                        agg_results['UP_PULSE_TIME'] = up_time if not pd.isnull(up_time) else None
                    elif agg == 'DOWN_PULSE_TIME':
                        down_time = df[df['value'].diff() < 0].index.max()
                        agg_results['DOWN_PULSE_TIME'] = down_time if not pd.isnull(down_time) else None
                    else:
                        raise ValueError(f"Unsupported aggregation type: {agg}")
                results[source_key] = agg_results
        
        return results, (datetime.now() - timedelta(seconds=delay)).strftime('%Y-%m-%dT%H:%M:%S')


    # ------------------------------
    # ğŸ“Œ ë°±ì—… ë° ë³µì› ê¸°ëŠ¥
    # ------------------------------

    def backup_data(self, file_path='redis_backup.json'):
        """Redis ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ë°±ì—…"""
        data = {key: self.get_value(key) for key in self.get_all_keys()}
        with open(file_path, 'w', encoding='utf-8') as file:
            json.dump(data, file, ensure_ascii=False, indent=4)

    def restore_data(self, file_path='redis_backup.json'):
        """JSON ë°±ì—… íŒŒì¼ì„ Redisë¡œ ë³µì›"""
        with open(file_path, 'r', encoding='utf-8') as file:
            data = json.load(file)
            for key, value in data.items():
                self.set_value(key, value)

    # ------------------------------
    # ğŸ“Œ ì‹œê³„ì—´ ë°ì´í„° Bulk ì €ì¥ ë° ì—…ë°ì´íŠ¸
    # ------------------------------

    def bulk_add_timeseries(self, data):
        """
        ì—¬ëŸ¬ ê°œì˜ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì¶”ê°€ (bulk_create ì—­í• )
        :param data: {
            "key1": [(timestamp1, value1), (timestamp2, value2), ...],
            "key2": [(timestamp1, value1), (timestamp2, value2), ...],
        }
        
        ğŸ“Œ ì‚¬ìš© ì˜ˆì‹œ:
        redis_manager.bulk_add_timeseries({
            "sensor:1": [(1672531200000, 25.5), (1672531300000, 26.1)],
            "sensor:2": [(1672531200000, 30.2), (1672531300000, 29.8)]
        })
        """
        pipeline = self.client.pipeline()
        for key, values in data.items():
            for timestamp, value in values:
                pipeline.execute_command("TS.ADD", key, timestamp, value)
        pipeline.execute()

    def bulk_update_timeseries(self, data):
        """
        ì—¬ëŸ¬ ê°œì˜ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì—…ë°ì´íŠ¸ (bulk_update ì—­í• )
        ê¸°ì¡´ ê°’ì´ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ì¶”ê°€
        :param data: {
            "key1": [(timestamp1, new_value1), (timestamp2, new_value2), ...],
            "key2": [(timestamp1, new_value1), (timestamp2, new_value2), ...],
        }

        ğŸ“Œ ì‚¬ìš© ì˜ˆì‹œ:
        redis_manager.bulk_update_timeseries({
            "sensor:1": [(1672531200000, 26.0), (1672531300000, 27.0)],
            "sensor:2": [(1672531200000, 31.0), (1672531300000, 30.5)]
        })
        """
        pipeline = self.client.pipeline()
        for key, values in data.items():
            for timestamp, new_value in values:
                existing_data = self.client.execute_command("TS.RANGE", key, timestamp, timestamp)
                
                if existing_data:
                    # ê°’ì´ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ì—…ë°ì´íŠ¸ (ì‚­ì œ í›„ ì¬ì¶”ê°€)
                    self.client.execute_command("TS.DEL", key, timestamp, timestamp)
                
                # ìƒˆë¡œìš´ ê°’ ì¶”ê°€
                pipeline.execute_command("TS.ADD", key, timestamp, new_value)
        pipeline.execute()


class AsyncRedisManager:
    """
    Redis ê¸°ë³¸ ë°ì´í„°, í•´ì‹œ ë°ì´í„°, ì‹œê³„ì—´ ë°ì´í„° ë° ë°±ì—…ì„ í†µí•© ê´€ë¦¬í•˜ëŠ” ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸

    async def main():
    redis_manager = AsyncRedisManager()
    await redis_manager.connect()

    # ë°ì´í„° ì €ì¥
    await redis_manager.set_value("test_key", {"name": "Alice", "age": 30})
    result = await redis_manager.get_value("test_key")
    print("ì¡°íšŒ ê²°ê³¼:", result)  # {"name": "Alice", "age": 30}

    # ì‹œê³„ì—´ ë°ì´í„° ì €ì¥
    await redis_manager.create_timeseries("sensor:1", retention=60000, labels={"location": "kitchen"})
    await redis_manager.add_timeseries_data("sensor:1", 22.5)

    # ì‹¤í–‰
    asyncio.run(main())
    """

    def __init__(self, host='localhost', port=6379, db=0, max_connections=20, password=None):
        """
        Redis ì—°ê²° ì´ˆê¸°í™” (ë¹„ë°€ë²ˆí˜¸ ì¸ì¦ í¬í•¨)
        """
        self.host = host
        self.port = port
        self.db = db
        self.max_connections = max_connections
        self.password = password
        self.client = None  # ë¹„ë™ê¸° Redis í´ë¼ì´ì–¸íŠ¸

    async def connect(self):
        """ Redis ì„œë²„ì— ë¹„ë™ê¸° ì—°ê²° """
        try:
            self.client = await aioredis.from_url(
                f"redis://{self.host}:{self.port}/{self.db}",
                password=self.password,
                decode_responses=True,
                max_connections=self.max_connections,
                socket_timeout=10,  # 10ì´ˆ í›„ íƒ€ì„ì•„ì›ƒ
                socket_connect_timeout=10,  # ì—°ê²° íƒ€ì„ì•„ì›ƒ 10ì´ˆ
            )
        except Exception as e:
            print(f"Redis ì—°ê²° ì˜¤ë¥˜: {e}")

    async def is_connected(self):
        """ì—°ê²° ìƒíƒœ í™•ì¸ (ping í…ŒìŠ¤íŠ¸)"""
        try:
            return await self.client.ping() if self.client else False
        except Exception:
            return False

    # ------------------------------
    # ğŸ“Œ ì¼ë°˜ ë°ì´í„° ì €ì¥ ë° ì¡°íšŒ (ë¹„ë™ê¸°)
    # ------------------------------

    async def set_value(self, key, value, expire=None):
        """ì¼ë°˜ ë°ì´í„° ì €ì¥"""
        if self.client is None:
            await self.connect()
        value = json.dumps(value)
        await self.client.set(key, value, ex=expire)

    async def get_value(self, key):
        """ì¼ë°˜ ë°ì´í„° ì¡°íšŒ"""
        if self.client is None:
            await self.connect()
        value = await self.client.get(key)
        return json.loads(value) if value else None

    async def delete_value(self, key):
        """íŠ¹ì • í‚¤ ì‚­ì œ"""
        if self.client is None:
            await self.connect()
        await self.client.delete(key)

    async def exists(self, key):
        """í‚¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        if self.client is None:
            await self.connect()
        return await self.client.exists(key) > 0

    async def flush(self):
        """ëª¨ë“  ë°ì´í„° ì‚­ì œ"""
        if self.client is None:
            await self.connect()
        await self.client.flushdb()

    async def get_all_keys(self):
        """ëª¨ë“  í‚¤ ëª©ë¡ ì¡°íšŒ"""
        if self.client is None:
            await self.connect()
        return await self.client.keys('*')

    async def mget(self, keys, as_dict=True):
        """ì—¬ëŸ¬ í‚¤ë¥¼ í•œ ë²ˆì— ì¡°íšŒ(MGET)í•˜ì—¬ JSON ë””ì½”ë”©í•´ ë°˜í™˜ (ë¹„ë™ê¸°)
        :param keys: ì¡°íšŒí•  í‚¤ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ë‹¨ì¼ í‚¤
        :param as_dict: Trueë©´ {key: value} dict, Falseë©´ ê°’ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        """
        if self.client is None:
            await self.connect()
        if isinstance(keys, (str, bytes)):
            keys = [keys]
        values = await self.client.mget(keys)
        decoded = []
        for v in values:
            try:
                decoded.append(json.loads(v) if v else None)
            except Exception:
                decoded.append(v)
        return {k: v for k, v in zip(keys, decoded)} if as_dict else decoded

    # ------------------------------
    # ğŸ“Œ Bulk ë°ì´í„° ì²˜ë¦¬ (ë¹„ë™ê¸°)
    # ------------------------------

    async def bulk_set(self, data, expire=None):
        """ ì—¬ëŸ¬ ê°œì˜ í‚¤-ê°’ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì €ì¥ """
        if self.client is None:
            await self.connect()
        async with self.client.pipeline() as pipe:
            for key, value in data.items():
                await pipe.set(key, json.dumps(value), ex=expire)
            await pipe.execute()

    async def bulk_update(self, data, expire=None):
        """ ì—¬ëŸ¬ ê°œì˜ í‚¤-ê°’ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì—…ë°ì´íŠ¸ """
        if self.client is None:
            await self.connect()
        async with self.client.pipeline() as pipe:
            for key, new_value in data.items():
                existing_value = await self.client.get(key)
                if existing_value:
                    updated_value = json.loads(existing_value)
                    if isinstance(updated_value, dict) and isinstance(new_value, dict):
                        updated_value.update(new_value)
                    else:
                        updated_value = new_value
                else:
                    updated_value = new_value
                await pipe.set(key, json.dumps(updated_value), ex=expire)
            await pipe.execute()

    # ------------------------------
    # ğŸ“Œ í•´ì‹œ ë°ì´í„° Bulk ì €ì¥ ë° ì—…ë°ì´íŠ¸ (ë¹„ë™ê¸°)
    # ------------------------------

    async def hbulk_set(self, name, data):
        """ ì—¬ëŸ¬ ê°œì˜ í•´ì‹œ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì €ì¥ """
        if self.client is None:
            await self.connect()
        async with self.client.pipeline() as pipe:
            for field, value in data.items():
                await pipe.hset(name, field, json.dumps(value))
            await pipe.execute()

    async def hbulk_update(self, name, data):
        """ ì—¬ëŸ¬ ê°œì˜ í•´ì‹œ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì—…ë°ì´íŠ¸ """
        if self.client is None:
            await self.connect()
        async with self.client.pipeline() as pipe:
            for field, new_value in data.items():
                existing_value = await self.client.hget(name, field)
                if existing_value:
                    updated_value = json.loads(existing_value)
                    if isinstance(updated_value, dict) and isinstance(new_value, dict):
                        updated_value.update(new_value)
                    else:
                        updated_value = new_value
                else:
                    updated_value = new_value
                await pipe.hset(name, field, json.dumps(updated_value))
            await pipe.execute()

    # ------------------------------
    # ğŸ“Œ í•´ì‹œ ë°ì´í„° ì €ì¥ ë° ì¡°íšŒ (ë¹„ë™ê¸°)
    # ------------------------------

    async def hset(self, name, key, value):
        """í•´ì‹œ ë°ì´í„° ì €ì¥"""
        if self.client is None:
            await self.connect()
        value = json.dumps(value)
        await self.client.hset(name, key, value)

    async def hget(self, name, key):
        """í•´ì‹œ ë°ì´í„° ì¡°íšŒ"""
        if self.client is None:
            await self.connect()
        value = await self.client.hget(name, key)
        return json.loads(value) if value else None

    async def hmset(self, name, mapping):
        """í•´ì‹œ ë°ì´í„° ì—¬ëŸ¬ ê°œ ì €ì¥"""
        if self.client is None:
            await self.connect()
        mapping = {key: json.dumps(value) for key, value in mapping.items()}
        await self.client.hset(name, mapping=mapping)

    async def hmget(self, name, keys):
        """í•´ì‹œ ë°ì´í„° ì—¬ëŸ¬ ê°œ ì¡°íšŒ"""
        if self.client is None:
            await self.connect()
        values = await self.client.hmget(name, keys)
        return {key: json.loads(value) if value else None for key, value in zip(keys, values)}

    async def hexists(self, name, key):
        """í‚¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        if self.client is None:
            await self.connect()
        return await self.client.hexists(name, key)

    async def hcreate_or_update(self, name, data):
        """í•´ì‹œ ë°ì´í„° ì—…ë°ì´íŠ¸ ë˜ëŠ” ìƒì„±"""
        if self.client is None:
            await self.connect()
        async with self.client.pipeline() as pipe:
            for key, value in data.items():
                existing_value = await self.client.hget(name, key)
                if existing_value:
                    updated_value = json.loads(existing_value)
                    if isinstance(updated_value, dict) and isinstance(value, dict):
                        updated_value.update(value)
                    else:
                        updated_value = value
                else:
                    updated_value = value
                await pipe.hset(name, key, json.dumps(updated_value))
            await pipe.execute()


    # ------------------------------
    # ğŸ“Œ ì¼ë°˜ í‚¤ ì—…ë°ì´íŠ¸ (ì—†ìœ¼ë©´ ìƒì„±)
    # ------------------------------
    async def create_or_update(self, key, value, expire=None):
        """
        ì¼ë°˜ í‚¤ ë°ì´í„° ì—…ë°ì´íŠ¸ ë˜ëŠ” ìƒì„± (ë¹„ë™ê¸°)
        
        :param key: Redis í‚¤
        :param value: ì €ì¥í•  ê°’ (dict)
        :param expire: ë§Œë£Œ ì‹œê°„ (ì´ˆ) (ì˜µì…˜)
        
        ì‚¬ìš© ì˜ˆì‹œ:
        await redis_manager.create_or_update("user:123", {"name": "Alice"}, expire=3600)
        """
        try:
            # ê¸°ì¡´ ê°’ ê°€ì ¸ì˜¤ê¸° (ë¹„ë™ê¸°)
            existing_value = await self.client.get(key)

            if existing_value:
                updated_value = json.loads(existing_value)  # ê¸°ì¡´ JSON ë°ì´í„°ë¥¼ dictë¡œ ë³€í™˜
                updated_value.update(value)  # ê¸°ì¡´ ë°ì´í„° ì—…ë°ì´íŠ¸
            else:
                updated_value = value  # ê¸°ì¡´ ê°’ì´ ì—†ìœ¼ë©´ ìƒˆë¡œìš´ ê°’ ì‚¬ìš©

            # JSON í˜•ì‹ìœ¼ë¡œ Redisì— ì €ì¥ (ë¹„ë™ê¸°)
            updated_value = json.dumps(updated_value)
            await self.client.set(key, updated_value, ex=expire)

        except RedisError as e:
            return {"error": f"Redis error: {str(e)}"}
        
    # ------------------------------
    # ğŸ“Œ ì‹œê³„ì—´ ë°ì´í„° ê´€ë¦¬ (TimeSeries)
    # ------------------------------

    async def create_timeseries(self, key, retention=0, labels=None):
        """ ì‹œê³„ì—´ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± """
        if labels is None:
            labels = {}

        if self.client is None:
            await self.connect()

        exists = await self.client.exists(key)
        label_args = []
        for label, value in labels.items():
            label_args.extend([label, value])

        if exists:
            await self.client.execute_command(
                'TS.ALTER', key, 'DUPLICATE_POLICY', 'LAST', 'RETENTION', retention, 'LABELS', *label_args
            )
        else:
            await self.client.execute_command(
                'TS.CREATE', key, 'DUPLICATE_POLICY', 'FIRST', 'RETENTION', retention, 'LABELS', *label_args
            )

    async def add_timeseries_data(self, key, value, timestamp=None):
        """ ì‹œê³„ì—´ ë°ì´í„° ì¶”ê°€ """
        if timestamp is None:
            timestamp = int(datetime.utcnow().timestamp() * 1000)

        if self.client is None:
            await self.connect()

        await self.client.execute_command('TS.ADD', key, timestamp, value)

    
    async def get_timeseries_range(self, key_pattern, start='-', end='+', count=None, callback=None):
        """
        íŠ¹ì • ê¸°ê°„ì˜ ì‹œê³„ì—´ ë°ì´í„° ì¡°íšŒ (ì™€ì¼ë“œì¹´ë“œ ì§€ì›) + COUNT ì˜µì…˜ ì¶”ê°€
        :param key_pattern: ê²€ìƒ‰í•  í‚¤ íŒ¨í„´ (ì˜ˆ: "alert:*")
        :param start: ì¡°íšŒ ì‹œì‘ ì‹œê°„ ('-'ëŠ” ê°€ì¥ ì˜¤ë˜ëœ ë°ì´í„°)
        :param end: ì¡°íšŒ ì¢…ë£Œ ì‹œê°„ ('+'ëŠ” ìµœì‹  ë°ì´í„°)
        :param count: ì¡°íšŒí•  ë°ì´í„° ê°œìˆ˜ ì œí•œ (Optional)
        :param callback: í›„ì²˜ë¦¬ ì½œë°± í•¨ìˆ˜ (Optional)['get_labels_callback']

        ì‚¬ìš© ì˜ˆì‹œ:
        data = await tsdb.get_timeseries_range("alert:*", start=1672531200000, end=1672617600000, count=100)
        print(data)
        """
        try:
            keys = []

            # ì™€ì¼ë“œì¹´ë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸
            if '*' in key_pattern:
                # SCAN ëª…ë ¹ì–´ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰í•˜ì—¬ ì¼ì¹˜í•˜ëŠ” í‚¤ ì°¾ê¸°
                cursor = 0
                while True:
                    cursor, found_keys = await self.client.scan(cursor, match=key_pattern, count=100)
                    keys.extend(found_keys)
                    if cursor == 0:
                        break
            else:
                keys.append(key_pattern)  # ë‹¨ì¼ í‚¤ ì¡°íšŒ

            if not keys:
                return {"error": "No matching TimeSeries keys found."}

            # Redis Pipeline ì‚¬ìš© (ë¹„ë™ê¸° ì²˜ë¦¬)
            pipeline = self.client.pipeline()
            for key in keys:
                if count:
                    pipeline.execute_command("TS.RANGE", key, start, end, "COUNT", count)
                else:
                    pipeline.execute_command("TS.RANGE", key, start, end)

            # ê²°ê³¼ ì‹¤í–‰ (ë¹„ë™ê¸°)
            results = await pipeline.execute()

            # ë°ì´í„° ì •ë¦¬
            data = {
                key: [{"timestamp": ts, "value": value} for ts, value in result]
                for key, result in zip(keys, results)
            }

            # ì½œë°±ì´ ì œê³µëœ ê²½ìš° ì‹¤í–‰ (ë¹„ë™ê¸° ì²˜ë¦¬)
            if callback:
                return await callback(data, keys)  # ì½œë°±ë„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            else:
                return data

        except RedisError as e:
            return {"error": f"Redis error: {str(e)}"}
        
    async def get_labels_callback(self, result, keys):
        """
        í´ë˜ìŠ¤ ë‚´ë¶€ì˜ ì½œë°± í•¨ìˆ˜ (ë°ì´í„°ë¥¼ ì¶”ê°€ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ)
        """
        try:
            if not keys:
                return result

            # Redis Pipeline ì‚¬ìš© (ë¹„ë™ê¸°)
            pipeline = self.client.pipeline()
            for key in keys:
                pipeline.execute_command("TS.INFO", key)
            
            info_results = await pipeline.execute()

            # ë¼ë²¨ ë°ì´í„° ì •ë¦¬
            labels_dict = {}
            for key, info in zip(keys, info_results):
                try:
                    # info ë¦¬ìŠ¤íŠ¸ì—ì„œ 'labels' í‚¤ ì°¾ê¸°
                    label_index = info.index("labels") if "labels" in info else -1
                    if label_index != -1 and label_index + 1 < len(info):
                        raw_labels = info[label_index + 1]  # ë¼ë²¨ ë¦¬ìŠ¤íŠ¸
                        if isinstance(raw_labels, list):
                            # ë¦¬ìŠ¤íŠ¸ ë‚´ ë¦¬ìŠ¤íŠ¸ êµ¬ì¡°ë¥¼ {key: value} í˜•íƒœë¡œ ë³€í™˜
                            labels_dict[key] = {str(entry[0]): str(entry[1]) for entry in raw_labels if isinstance(entry, list) and len(entry) == 2}
                        else:
                            labels_dict[key] = {}
                    else:
                        labels_dict[key] = {}

                except ValueError:
                    labels_dict[key] = {}  # ì˜ˆì™¸ ë°œìƒ ì‹œ ë¹ˆ ë¼ë²¨ ì €ì¥

            return result, labels_dict  # ë¼ë²¨ ì •ë³´ í¬í•¨í•˜ì—¬ ë°˜í™˜

        except RedisError as e:
            return result

    async def timeseries_exists(self, key):
        """
        Redis Time Series ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜
        """
        try:
            # TS.INFOë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ì˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ë¹„ë™ê¸°)
            totalSamples = await self.client.execute_command('TS.INFO', key)
            if totalSamples[1] > 0:
                return True  # ë°ì´í„°ê°€ ì¡´ì¬í•˜ë©´ True ë°˜í™˜
            else:
                return False  # ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ False ë°˜í™˜
        except Exception as e:
            if "TSDB: the key does not exist" in str(e):
                return False  # ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ False ë°˜í™˜
            else:
                raise  # ê¸°íƒ€ ì˜¤ë¥˜ëŠ” ê·¸ëŒ€ë¡œ ë°œìƒ

    
    async def get_latest_timeseries(self, key, callback=None):
        """
        ìµœì‹  ì‹œê³„ì—´ ë°ì´í„° ì¡°íšŒ + ë¼ë²¨ ì •ë³´ í¬í•¨ (ì˜µì…˜)
        :param callback ['get_labels_callback']
        
        ì‚¬ìš© ì˜ˆì‹œ:
        latest = await tsdb.get_latest_timeseries("sensor:1")
        print(latest)
        """
        try:
            # ìµœì‹  ë°ì´í„° ì¡°íšŒ (ë¹„ë™ê¸° ì‹¤í–‰)
            result = await self.client.execute_command('TS.GET', key)
            if not result:
                return {"error": "TimeSeries key not found."}

            # ì½œë°±ì´ ì œê³µëœ ê²½ìš° ì‹¤í–‰
            if callback:
                return await callback(result, [key])  # ì½œë°±ë„ ë¹„ë™ê¸° ì²˜ë¦¬
            else:
                return result

        except RedisError as e:
            return {"error": f"Redis error: {str(e)}"}

    async def get_pattern_latest_timeseries(self, key_pattern, callback=None):
        """
        ì™€ì¼ë“œì¹´ë“œ íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ì‹œê³„ì—´ í‚¤ì˜ ìµœì‹  ë°ì´í„° ì¡°íšŒ
        :param callback ['get_labels_callback']

        ì‚¬ìš© ì˜ˆì‹œ:
        latest = await tsdb.get_pattern_latest_timeseries("sensor:*")
        print(latest)
        """
        try:
            # íŒ¨í„´ì— '*'ê°€ ì—†ìœ¼ë©´ ì˜ˆì™¸ ì²˜ë¦¬
            if '*' not in key_pattern:
                return {"error": "This function only supports wildcard patterns. Use get_latest_timeseries for single keys."}

            keys = []
            cursor = 0

            # SCANì„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰í•˜ì—¬ íŒ¨í„´ì— ë§ëŠ” í‚¤ ê²€ìƒ‰
            while True:
                cursor, found_keys = await self.client.scan(cursor, match=key_pattern, count=100)
                keys.extend(found_keys)
                if cursor == 0:
                    break

            if not keys:
                return {"error": "No matching TimeSeries keys found."}

            # Redis Pipeline ì‚¬ìš© (ë¹„ë™ê¸° ì²˜ë¦¬)
            pipeline = self.client.pipeline()
            for key in keys:
                pipeline.execute_command("TS.GET", key)

            results = await pipeline.execute()  # ë¹„ë™ê¸°ë¡œ ì‹¤í–‰

            # ë°ì´í„° ì •ë¦¬
            data = {
                key: {"timestamp": result[0], "value": result[1]} if result else None
                for key, result in zip(keys, results)
            }

            # ì½œë°±ì´ ì œê³µëœ ê²½ìš° ì‹¤í–‰
            if callback:
                return await callback(data, keys)  # ì½œë°±ë„ ë¹„ë™ê¸° ì²˜ë¦¬
            else:
                return data

        except RedisError as e:
            return {"error": f"Redis error: {str(e)}"}

    async def delete_timeseries(self, key):
        """
        ì‹œê³„ì—´ í‚¤ ì‚­ì œ
        
        ì‚¬ìš© ì˜ˆì‹œ:
        await tsdb.delete_timeseries("sensor:1")
        """
        try:
            return await self.client.delete(key)
        except RedisError as e:
            return {"error": f"Redis error: {str(e)}"}


    # ------------------------------
    # ğŸ“Œ í‚¤ ê²€ìƒ‰ ê¸°ëŠ¥ (ë¹„ë™ê¸°)
    # ------------------------------

    async def query_scan(self, pattern):
        """ Redisì˜ ëª¨ë“  í‚¤ë¥¼ SCANì„ ì´ìš©í•´ ê²€ìƒ‰ """
        if self.client is None:
            await self.connect()
        cursor = 0
        matched_keys = []

        while True:
            cursor, keys = await self.client.scan(cursor, match=pattern, count=100)
            if keys:
                matched_keys.extend(keys)
            if cursor == 0:
                break

        return matched_keys

    async def query_keys(self, pattern):
        """
        íŒ¨í„´ì„ ê¸°ë°˜ìœ¼ë¡œ íŠ¹ì • í‚¤ë§Œ ì¡°íšŒ (ë¹„ë™ê¸°)
        :param pattern: ["*", "9", "Sensor", "*", "*"] ê°™ì€ ë¦¬ìŠ¤íŠ¸ í˜•ì‹
        ì‚¬ìš© ì˜ˆì‹œ:
        result_keys = await redis_manager.query_keys(["*", "9", "Sensor", "*", "*"])
        print(result_keys)
        """
        try:
            # 1ï¸âƒ£ ëª¨ë“  í‚¤ ê°€ì ¸ì˜¤ê¸° (ë¹„ë™ê¸°)
            keys = await self.client.keys('*')

            # 2ï¸âƒ£ "*"ì„ ì œì™¸í•œ í•„í„°ë§í•  ê°’ë§Œ ì¶”ì¶œ
            filters = [p for p in pattern if p != "*"]

            # 3ï¸âƒ£ í•„í„°ë§ ì ìš© (ëª¨ë“  í‚¤ì—ì„œ í•„í„°ë§í•  ê°’ì´ ìˆëŠ”ì§€ í™•ì¸)
            result_keys = [key for key in keys if all(f in key for f in filters)]

            return result_keys

        except RedisError as e:
            return {"error": f"Redis error: {str(e)}"}

    async def query_by_label(self, label_filter):
        """ ë¼ë²¨ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹œê³„ì—´ í‚¤ ê²€ìƒ‰ """
        if self.client is None:
            await self.connect()
        return await self.client.execute_command('TS.QUERYINDEX', label_filter)

    async def get_data_by_label(self, label_filter, start='-', end='+'):
        """ ë¼ë²¨ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹œê³„ì—´ ë°ì´í„° ì¡°íšŒ """
        if self.client is None:
            await self.connect()
        return await self.client.execute_command('TS.MRANGE', start, end, 'FILTER', label_filter)

    # ------------------------------
    # ğŸ“Œ ì»¤ìŠ¤í…€ ì§‘ê³„ ê¸°ëŠ¥ (ë¹„ë™ê¸°)
    # ------------------------------

    async def custom_aggregate(self, source_keys, query_key='*', aggregate_type=['MIN', 'MAX', 'AVG'], batch_size=10, minute=5, delay=0):
        """
        ë¹„ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì—¬ëŸ¬ í‚¤ì˜ ì§‘ê³„ ì—°ì‚° ìˆ˜í–‰
        """
        if self.client is None:
            await self.connect()
        
        results = {}
        ndigit = 4
        end_time = int(datetime.now().timestamp() * 1000) - delay * 1000
        start_time = end_time - (minute * 60 * 1000)
        total_time = (end_time - start_time) / 1000
        
        for i in range(0, len(source_keys), batch_size):
            batch_keys = [key for key in source_keys[i:i + batch_size] if query_key in key or query_key == '*']
            async with self.client.pipeline() as pipe:
                for source_key in batch_keys:
                    await pipe.execute_command('TS.RANGE', source_key, start_time, end_time)
                responses = await pipe.execute()
            
            for source_key, data in zip(batch_keys, responses):
                if not data:
                    results[source_key] = {agg: None for agg in aggregate_type}
                    continue
                
                df = pd.DataFrame(data, columns=['timestamp', 'value'])
                df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
                df.set_index('timestamp', inplace=True)
                
                # ê°’ì´ ë¬¸ìì—´ë¡œ ë°˜í™˜ë  ê²½ìš° ìˆ«ìë¡œ ë³€í™˜
                df['value'] = pd.to_numeric(df['value'], errors='coerce')
                df.dropna(subset=['value'], inplace=True)  # ë³€í™˜ë˜ì§€ ì•Šì€ ê°’ ì œê±°
                
                if df.empty:
                    results[source_key] = {agg: None for agg in aggregate_type}
                    continue
                
                # ë°ì´í„° íƒ€ì… í™•ì¸
                dtype = df['value'].dtype

                # int, float, bool ì´ì™¸ì˜ ê²½ìš° (ì˜ˆ: ë¬¸ìì—´) ì˜ˆì™¸ ì²˜ë¦¬
                if not np.issubdtype(dtype, np.number):
                    raise ValueError("value ì»¬ëŸ¼ì´ ìˆ«ìí˜•ì´ ì•„ë‹™ë‹ˆë‹¤.")
                
                agg_results = {}
                for agg in aggregate_type:
                    if agg == 'AVG':  # í‰ê· ê°’ ê³„ì‚°
                        agg_results['AVG'] = round(float(df['value'].mean()), ndigit) if np.issubdtype(dtype, np.floating) else int(df['value'].mean())
                    elif agg == 'SUM':  # í•©ê³„ ê³„ì‚°
                        agg_results['SUM'] = round(float(df['value'].sum()), ndigit) if np.issubdtype(dtype, np.floating) else int(df['value'].sum())
                    elif agg == 'MIN':  # ìµœì†Œê°’ ê³„ì‚°
                        agg_results['MIN'] = round(float(df['value'].min()), ndigit) if np.issubdtype(dtype, np.floating) else int(df['value'].min())
                    elif agg == 'MAX':  # ìµœëŒ€ê°’ ê³„ì‚°
                        agg_results['MAX'] = round(float(df['value'].max()), ndigit) if np.issubdtype(dtype, np.floating) else int(df['value'].max())
                    elif agg == 'COUNT':  # ë°ì´í„° ê°œìˆ˜ ê³„ì‚°
                        agg_results['COUNT'] = df['value'].count()
                    elif agg == 'FIRST':  # ì²« ë²ˆì§¸ ê°’
                        agg_results['FIRST'] = round(float(df['value'].iloc[0]), ndigit) if np.issubdtype(dtype, np.floating) else int(df['value'].iloc[0])
                    elif agg == 'LAST':  # ë§ˆì§€ë§‰ ê°’
                        agg_results['LAST'] = round(float(df['value'].iloc[-1]), ndigit) if np.issubdtype(dtype, np.floating) else int(df['value'].iloc[-1])
                    elif agg == 'STDDEV':
                        agg_results['STDDEV'] = round(float(df['value'].std()), ndigit)  # ğŸ”¹ float ë³€í™˜
                    elif agg == 'VARIANCE':
                        agg_results['VARIANCE'] = round(float(df['value'].var()), ndigit)  # ğŸ”¹ float ë³€í™˜
                    elif agg == 'RANGE':
                        agg_results['RANGE'] = round(float(df['value'].max() - df['value'].min()), ndigit) if np.issubdtype(dtype, np.floating) else int(df['value'].max() - df['value'].min())
                    elif agg == 'DIFFERENCE':
                        agg_results['DIFFERENCE'] = [round(float(x), ndigit) for x in df['value'].diff().dropna().tolist()]  # ğŸ”¹ ë¦¬ìŠ¤íŠ¸ ë‚´ ê°’ float ë³€í™˜
                    elif agg == 'DIFFERENCE_SUM':
                        diff_sum = df['value'].diff().dropna().sum()
                        agg_results['DIFFERENCE_SUM'] = round(diff_sum, ndigit) if diff_sum is not None else 0  # ğŸ”¹ float ë³€í™˜
                    elif agg == 'DERIVATIVE':
                        diff_mean = df['value'].diff().dropna().mean()
                        # NaNì´ ë‚˜ì˜¤ì§€ ì•Šë„ë¡ ì²´í¬ í›„ ë³€í™˜
                        agg_results['DERIVATIVE'] = round(float(diff_mean), ndigit) if not np.isnan(diff_mean) else 0
                    elif agg == 'NONNEGATIVE_DERIVATIVE':
                        agg_results['NONNEGATIVE_DERIVATIVE'] = round(float(df['value'].diff().clip(lower=0).dropna().mean()), ndigit)  # ğŸ”¹ float ë³€í™˜
                    elif agg == 'HISTOGRAM':
                        agg_results['HISTOGRAM'] = {k: int(v) for k, v in df['value'].value_counts().to_dict().items()}  # ğŸ”¹ int ë³€í™˜
                    elif agg == 'SPREAD':
                        agg_results['SPREAD'] = round(float(df['value'].max() - df['value'].min()), ndigit)  # ğŸ”¹ float ë³€í™˜
                    elif agg == 'MEDIAN':
                        agg_results['MEDIAN'] = round(float(df['value'].median()), ndigit)  # ğŸ”¹ float ë³€í™˜
                    elif agg == 'SKEW':
                        agg_results['SKEW'] = round(float(df['value'].skew()), ndigit)  # ğŸ”¹ float ë³€í™˜
                    elif agg == 'CHANGE_TIME':  # ë§ˆì§€ë§‰ìœ¼ë¡œ ê°’ì´ ë³€í•œ ì‹œì 
                        last_change = df[df['value'] != df['value'].shift()].index[-1]
                        # UTC íƒ€ì„ì¡´ ì§€ì • í›„, ë¡œì»¬ íƒ€ì„ì¡´ ë³€í™˜
                        last_change = last_change.tz_localize('UTC').tz_convert(local_tz)
                        agg_results['CHANGE_TIME'] = last_change
                    elif agg == 'RUN_TIME':  # ê°’ì´ 0ì´ ì•„ë‹Œ ì‹œê°„ì˜ ì´í•©
                        run_time_series = df[df['value'] > 0].index.to_series().diff().dropna().dt.total_seconds()
                        run_time = run_time_series.sum() if not run_time_series.empty else 0
                        agg_results['RUN_TIME'] = run_time
                    elif agg == 'DOWN_TIME':  # ê°’ì´ 0ì¸ ì‹œê°„ì˜ ì´í•©
                        down_time = total_time - agg_results.get('RUN_TIME', 0)
                        agg_results['DOWN_TIME'] = down_time
                    elif agg == 'UP_PULSE_TIME':
                        up_time = df[df['value'].diff() > 0].index.max()
                        agg_results['UP_PULSE_TIME'] = up_time if not pd.isnull(up_time) else None
                    elif agg == 'DOWN_PULSE_TIME':
                        down_time = df[df['value'].diff() < 0].index.max()
                        agg_results['DOWN_PULSE_TIME'] = down_time if not pd.isnull(down_time) else None
                    else:
                        raise ValueError(f"Unsupported aggregation type: {agg}")
                results[source_key] = agg_results
        
        return results, (datetime.now() - timedelta(seconds=delay)).strftime('%Y-%m-%dT%H:%M:%S')
    
    # ------------------------------
    # ğŸ“Œ ë°±ì—… ë° ë³µì› ê¸°ëŠ¥ (ë¹„ë™ê¸°)
    # ------------------------------

    async def backup_data(self, file_path='redis_backup.json'):
        """ Redis ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ë°±ì—… """
        if self.client is None:
            await self.connect()
        data = {key: await self.get_value(key) for key in await self.get_all_keys()}
        with open(file_path, 'w', encoding='utf-8') as file:
            json.dump(data, file, ensure_ascii=False, indent=4)

    async def restore_data(self, file_path='redis_backup.json'):
        """ JSON ë°±ì—… íŒŒì¼ì„ Redisë¡œ ë³µì› """
        with open(file_path, 'r', encoding='utf-8') as file:
            data = json.load(file)
            for key, value in data.items():
                await self.set_value(key, value)
    # ------------------------------
    # ğŸ“Œ ì‹œê³„ì—´ ë°ì´í„° Bulk ì €ì¥ ë° ì—…ë°ì´íŠ¸ (ë¹„ë™ê¸°)
    # ------------------------------

    async def bulk_add_timeseries(self, data):
        """ ì—¬ëŸ¬ ê°œì˜ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì¶”ê°€ """
        if self.client is None:
            await self.connect()
        async with self.client.pipeline() as pipe:
            for key, values in data.items():
                for timestamp, value in values:
                    await pipe.execute_command("TS.ADD", key, timestamp, value)
            await pipe.execute()

    async def bulk_update_timeseries(self, data):
        """ ì—¬ëŸ¬ ê°œì˜ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì—…ë°ì´íŠ¸ """
        if self.client is None:
            await self.connect()
        async with self.client.pipeline() as pipe:
            for key, values in data.items():
                for timestamp, new_value in values:
                    existing_data = await self.client.execute_command("TS.RANGE", key, timestamp, timestamp)
                    if existing_data:
                        await self.client.execute_command("TS.DEL", key, timestamp, timestamp)
                    await pipe.execute_command("TS.ADD", key, timestamp, new_value)
            await pipe.execute()

    async def close(self):
        """Redis ì—°ê²° ì¢…ë£Œ"""
        if self.client:
            await self.client.close()
            print("âœ… Redis ì—°ê²° ì¢…ë£Œë¨")