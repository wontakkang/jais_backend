# config.py
"""
ëª¨ë¸ ê¸°ë³¸ ì„¤ì •ê°’ ê´€ë¦¬
- ëª¨ë¸ë³„ ê¸°ë³¸ íŒŒë¼ë¯¸í„°
- ëª¨ë¸ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë²”ìœ„
- ë°ì´í„° ì»¬ëŸ¼ ë° ê²½ë¡œ ì„¤ì •
"""

# ================================
# 0. ì„ì‹œ settings (Jupyter Notebook ì „ìš©)
# ================================

# AI Logger ì„¤ì •
import numpy as np
from utils.AI.ensemble_forecaster.logger import setup_logger
from utils.config import settings
import os
ensembleForecaster_logger = setup_logger(name="EnsembleForecaster_logger", log_file=f"{settings.LOG_DIR}/EnsembleForecaster.log")

class DefaultConfig:
    # ì˜ˆì¸¡ ëª¨ë¸ì— ì ìš©ë  ìƒ˜í”Œ ê°€ì¤‘ì¹˜ í•¨ìˆ˜
    def sample_weight_func(self, X, y, scale=2.0):
        """
        ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì‹œ ì‚¬ìš©í•  ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ìƒì„± í•¨ìˆ˜.

        Parameters:
            X (array-like): ì…ë ¥ ë°ì´í„° (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
            y (array-like): íƒ€ê¹ƒ ë²¡í„°
            scale (float): ê°€ì¤‘ì¹˜ ìƒí•œ (1.0 ~ scale ì‚¬ì´ ê°’ìœ¼ë¡œ ìƒì„±)

        Returns:
            np.ndarray: ê° ìƒ˜í”Œì— ëŒ€í•œ ê°€ì¤‘ì¹˜ ë²¡í„°
        """
        weights = np.random.uniform(1.0, scale, len(y))

        # ê°€ì¤‘ì¹˜ í†µê³„ ë¡œê¹…
        ensembleForecaster_logger.info(f"[sample_weight_func] ìƒ˜í”Œ ìˆ˜: {len(y)},  ê°€ì¤‘ì¹˜ ë²”ìœ„: min={weights.min():.4f}, max={weights.max():.4f},  ê°€ì¤‘ì¹˜ í‰ê· : mean={weights.mean():.4f}, í‘œì¤€í¸ì°¨: std={weights.std():.4f}")

        return weights
    
    def __init__(self):
        
        # ì „ì²˜ë¦¬ ê´€ë ¨ ì„¤ì •
        self.row_size = '5m'                            # ë°ì´í„° ìƒ˜í”Œë§ ì£¼ê¸°
        self.ref_interval = 5                          # ë°ì´í„° ìƒ˜í”Œë§ ì£¼ê¸°
        self.step_size = int(60/self.ref_interval) * 24    # step size
        self.input_seq_len = self.step_size * 7         # í•™ìŠµ ë°ì´í„° ê¸¸ì´
        self.output_seq_len = self.step_size * 3        # ì˜ˆì¸¡ ë°ì´í„° ê¸¸ì´
        self.cpu_count = int(os.cpu_count()*0.8)  # CPU ì½”ì–´ ìˆ˜ (80% ì‚¬ìš©)

        # ë°ì´í„° ê´€ë ¨ ì„¤ì •, *settings.NETWORK.keys()
        self.tag_datetime = 'READ_DATETIME'
        self.target_columns = [*settings.PWR_WM.keys()]
        self.feature_columns = [*settings.FEATURES.keys()]
        self.target_labels = {**settings.PWR_WM}
        self.filtered_columns = []
        
        # ë¨¸ì‹  ëŸ¬ë‹ ëª¨ë¸ ê¸°ë³¸ íŒŒë¼ë¯¸í„°
        self.catboost_params = self._catboost_default()
        self.lgbm_params = self._lgbm_default()
        self.xgb_params = self._xgboost_default()
        
        # ì•™ìƒë¸” ë¨¸ì‹  ëŸ¬ë‹ ëª¨ë¸ ëª¨ë¸ ê¸°ë³¸ íŒŒë¼ë¯¸í„°
        self.gradientboost_params = self._gradientboost_default()
        self.ml_model_list = ["lgbm", "catboost", "xgboost", "gbm"]
        self.class_weight = 'balanced'
    
        # ë”¥ ëŸ¬ë‹ ëª¨ë¸ ê¸°ë³¸ íŒŒë¼ë¯¸í„°
        self.cnn_params = self._cnn_default()
        self.lstm_params = self._lstm_default()
        self.cnn_lstm_params = self._cnn_lstm_default()
        self.tft_params = self._tft_default()
        self.cnn_tft_params = self._cnn_tft_default()
        self.patchtst_params = self._patchtst_default()
        self.informer_params = self._informer_default()
        
        # ì¬í•™ìŠµ í‰ê°€ì§€í‘œ ë¦¬ìŠ¤íŠ¸
        self.retrain_metrics = ['mse', 'mae', 'smape', 'r2']  # ì¬í•™ìŠµ í‰ê°€ì§€í‘œ ë¦¬ìŠ¤íŠ¸ ì¶”ê°€

        # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë²”ìœ„
        self.hyperparameter_grids = self._hyperparameter_defaults()

        # ì €ì¥ ê²½ë¡œ ì„¤ì •
        self.base_path = settings.BASE_PATH
        self.model_save_path = f'{self.base_path}model/'
        self.scaler_save_path = f'{self.base_path}scaler/'
        self.report_save_path = f'{self.base_path}report/'
        self.hyperparameters_save_path = f'{self.base_path}hyperparameters/'
        self.dataPattern_save_path = f'{self.base_path}dataPatterns/'
        self.features_save_path = f'{self.base_path}fitered_features/'

        # Optuna íŒŒë¼ë¯¸í„° ì„¤ì •
        self.optuna_neighborhood_ratio = {
            'catboost': 1.0,
            'lgbm': 1.5,
            'xgboost': 1.0,
            'tft': 2.0,
            'cnntft': 2.0,
            'patchtst': 2.0,
            'informer': 2.0,
            'lstm': 2.0,
            'cnnlstm': 2.0,
            'gbm': 1.0,
        }
        self.optuna_neighborhood_params = {
            'catboost': ['learning_rate', 'depth', 'l2_leaf_reg', 'bagging_temperature', 'random_strength', 'leaf_estimation_iterations'],
            'lgbm': ['learning_rate', 'num_leaves', 'max_depth', 'min_data_in_leaf', 'feature_fraction', 'bagging_fraction', 'lambda_l1', 'lambda_l2'],
            'xgboost': ['learning_rate', 'max_depth', 'min_child_weight', 'subsample', 'colsample_bytree', 'gamma', 'lambda', 'alpha'],
            'tft': ['num_heads', 'hidden_size', 'dropout_rate', 'learning_rate'],
            'cnntft': ['conv_filters', 'attention_heads', 'transformer_units', 'hidden_units', 'dropout_rate', 'learning_rate'],
            'patchtst': ['hidden_units', 'd_model', 'patch_len', 'stride', 'learning_rate'],
            'informer': ['hidden_units', 'd_model', 'n_heads', 'dropout_rate', 'factor', 'learning_rate'],
            'lstm': ['hidden_units', 'num_layers', 'lstm_units', 'dropout_rate', 'learning_rate'],
            'cnnlstm': ['conv_filters', 'lstm_units', 'hidden_units', 'dropout_rate', 'learning_rate'],
            'gbm': ['learning_rate', 'n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'subsample'],
        }

    def _cnn_default(self):
        return {
            'conv_filters': 128,         # Conv1D í•„í„° ìˆ˜
            'hidden_units': 512,          # Dense ìœ ë‹› ìˆ˜
            'learning_rate': 1e-3,        # í•™ìŠµë¥ 
            'loss': 'mse'                 # ì†ì‹¤ í•¨ìˆ˜
        }
    def _lstm_default(self):
        return {
            'lstm_units': 64,
            'dropout_rate': 0.2,
            'learning_rate': 0.001,
            'optimizer': 'adam',
            'loss': 'mse',
            'metrics': ['mae'],
        }
    def _cnn_lstm_default(self):
        return {
            'conv_filters': 64,
            'lstm_units': 64,
            'hidden_units': 128,
            'dropout_rate': 0.2,
            'learning_rate': 1e-3,
            'loss': 'mse'
        }
    def _tft_default(self):
        return {
            'num_heads': 4,
            'hidden_size': 64,
            'dropout_rate': 0.1,
            'learning_rate': 0.001,
            'optimizer': 'adam',
            'loss': 'mse',
            'metrics': ['mae'],
        }
    def _cnn_tft_default(self):
        return {
            'conv_filters': 64,         # Conv1D í•„í„° ê°œìˆ˜
            'attention_heads': 4,       # MultiHead Attention í—¤ë“œ ìˆ˜
            'transformer_units': 128,   # Transformer FeedForward ì¸µ ìœ ë‹› ìˆ˜
            'hidden_units': 128,        # Dense ì¸µ ìœ ë‹› ìˆ˜
            'dropout_rate': 0.1,        # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
            'learning_rate': 1e-3,      # í•™ìŠµë¥ 
            'loss': 'mse'               # ì†ì‹¤ í•¨ìˆ˜ (MSE ë˜ëŠ” Huber)
        }

    # ğŸ”¹ PatchTST ê¸°ë³¸ ì„¤ì •
    def _patchtst_default(self):
        return {
            'd_model': 128,
            'n_heads': 4,
            'patch_len': 16,
            'stride': 8,
            'dropout_rate': 0.2,
            'learning_rate': 0.001,
            'optimizer': 'adam',
            'loss': 'mse',
            'metrics': ['mae'],
        }

    # ğŸ”¹ Informer ê¸°ë³¸ ì„¤ì •
    def _informer_default(self):
        return {
            'd_model': 128,
            'n_heads': 4,
            'dropout_rate': 0.1,
            'factor': 5,              # Informer-specific
            'learning_rate': 0.001,
            'optimizer': 'adam',
            'loss': 'mse',
            'metrics': ['mae'],
        }

    def _catboost_default(self):
        return {
            'loss_function': "MultiRMSE",
            'eval_metric': "MultiRMSE",
            'thread_count': 12,
            'random_seed': 42,
            'grow_policy': 'Lossguide',
            'early_stopping_rounds': 50,
            'verbose': 0,
        }

    def _lgbm_default(self):
        return {
            # 'boosting_type': "gbdt",
            # 'device': "gpu",
            'n_jobs': self.cpu_count,
            'max_bin': 255,
            'verbosity': -1,
            'min_gain_to_split': 0.0,      # ì´ë“ ì¡°ê±´ ì™„í™”
        }

    def _xgboost_default(self):
        return {
            'verbosity': 0,  # ë¡œê·¸ ì¶œë ¥ ìˆ˜ì¤€. 0: ì—†ìŒ, 1: ì •ë³´, 2: ê²½ê³ , 3: ë””ë²„ê¹…
            # "tree_method": "hist",
            # "device": "cuda",
            # 'tree_method': 'hist',  # CPU ìŠ¤ë ˆë“œ ê°œìˆ˜ ì„¤ì • (8ì½”ì–´ ì‚¬ìš©)
            'n_jobs': self.cpu_count,
            # ëª¨ë¸ í•™ìŠµ ê´€ë ¨
            "objective": "reg:squarederror",  # íšŒê·€ ë¬¸ì œ
            "eval_metric": ["rmse", "mae"],  # íšŒê·€ ì„±ëŠ¥ í‰ê°€ ì§€í‘œ
            # "eval_metric": ["logloss", "auc"],  # ë¶„ë¥˜ ì„±ëŠ¥ í‰ê°€ ì§€í‘œ
        }

    def _gradientboost_default(self):
        return {
            'random_state': 42,
        }

    def _hyperparameter_defaults(self):
        return {
            'CatboostModel': {
                'objective': ['MultiRMSE'],
                'feature_border_type': ['Median', 'Uniform'],
                'grow_policy': ['SymmetricTree', 'Depthwise'],
                'depth': list(range(3, 13)),
                'learning_rate': [0.0025, 0.005, 0.01, 0.02, 0.025, 0.035, 0.05, 0.075, 0.1, 0.15, 0.2],
                'n_estimators': list(range(300, 1600, 100)),
                'bagging_temperature': [0.01, 0.05, 0.1, 0.2, 0.35, 0.5, 0.75, 1.0, 1.5, 2.0, 2.5, 3.0],
                'random_strength': [0.05, 0.1, 0.2, 0.3, 0.5, 0.7, 1.0, 1.5, 2.0, 3.0],
                'l2_leaf_reg': [0.05, 0.1, 0.3, 0.5, 0.75, 1.0, 2.0, 3.0, 5.0, 7.0, 10.0, 15.0, 20.0],
                'border_count': [32, 64, 128, 255],
                'bootstrap_type': ['Bayesian'],
                'leaf_estimation_method': ['Newton', 'Gradient'],
                'leaf_estimation_iterations': [1, 3, 5, 10, 15, 20],
            },
            'LgbmModel': {
                'task': ['train'],
                'tree_learner': ['feature'],
                'objective': ['regression', 'huber', 'fair', 'poisson'],
                'metric': ['rmse', 'mape', 'mae'],
                'num_leaves': [15, 25, 31, 40, 50, 75, 100, 125, 150],
                'learning_rate': [0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2, 0.25, 0.3],
                'n_estimators': list(range(300, 1600, 100)),
                'max_depth': [-1, 3, 4, 5, 6, 7, 8, 9, 10],
                'min_data_in_leaf': [5, 10, 15, 20, 30, 50],
                'min_child_samples': [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],
                'feature_fraction': [0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
                'bagging_fraction': [0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
                'bagging_freq': [1, 3, 5],
                'lambda_l1': [0.0, 0.01, 0.05, 0.1, 0.2, 0.35, 0.5, 1.0],
                'lambda_l2': [0.0, 0.01, 0.05, 0.1, 0.2, 0.35, 0.5, 1.0],
                'colsample_bynode': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
                'importance_type': ["gain"],
            },
            'XgboostModel': {
                'max_depth': list(range(3, 13)),
                'learning_rate': [0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],
                'n_estimators': list(range(250, 1600, 100)),
                'min_child_weight': [1, 2, 3, 5, 7, 10],
                'subsample': [0.3, 0.5, 0.7, 0.8, 0.9, 1.0],
                'colsample_bytree': [0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
                'gamma': [0, 0.01, 0.05, 0.1, 0.2, 0.3, 0.5, 0.7, 1.0],
                'lambda': [0, 0.01, 0.1, 0.5, 1.0, 2.0, 3.0, 5.0, 7.0, 10.0],
                'alpha': [0, 0.01, 0.1, 0.5, 1.0, 2.0, 3.0, 5.0, 7.0, 10.0],
            },
            'TFTModel': {
                'num_heads': [2, 4, 6, 8],
                'hidden_size': [32, 64, 128],
                'dropout_rate': [0.1, 0.2, 0.3],
                'learning_rate': [0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],
                'loss': ['mse', 'huber'],
            },
            'CNNTFTModel': {
                'conv_filters': [32, 64, 128],
                'attention_heads': [2, 4, 8],
                'transformer_units': [64, 128, 256],
                'hidden_units': [64, 128, 256],
                'dropout_rate': [0.0, 0.1, 0.2, 0.3],
                'learning_rate': [0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],
                'loss': ['mse', 'huber']
            },
            'PatchTSTModel': {
                'hidden_units': [32, 64, 128, 256, 512],
                'd_model': [64, 128, 256],
                'patch_len': [8, 16, 32],
                'stride': [4, 8, 16],
                'learning_rate': [0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],
                'loss': ['mse', 'huber'],
            },
            'InformerModel': {
                'hidden_units': [32, 64, 128, 256, 512],
                'd_model': [64, 128, 256],
                'n_heads': [2, 4, 8],
                'dropout_rate': [0.1, 0.2],
                'factor': [3, 5, 7],  # Informer-specific
                'learning_rate': [0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],
                'loss': ['mse', 'huber'],
            },
            'LSTMModel': {
                'hidden_units': [32, 64, 128, 256, 512],
                'num_layers': [1, 2, 3],
                'lstm_units': [32, 64, 128, 256],
                'dropout_rate': [0.0, 0.1, 0.3, 0.5],
                'learning_rate': [0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],
                'loss': ['mse', 'huber'],
            },
            'CNNLSTMModel': {
                'conv_filters': [32, 64, 128],
                'lstm_units': [32, 64, 128, 256],
                'hidden_units': [64, 128, 256],
                'dropout_rate': [0.0, 0.1, 0.2, 0.3],
                'learning_rate': [0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],
                'loss': ['mse', 'huber']
            },
            'GbmModel': {
                'loss': ['squared_error', 'absolute_error', 'huber', 'quantile'],  # ì†ì‹¤ í•¨ìˆ˜ ì„ íƒ
                'learning_rate': [0.0025, 0.005, 0.0075, 0.01, 0.015, 0.025, 0.035, 0.05, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2, 0.25],  # í•™ìŠµë¥  ì¡°ì •
                'n_estimators': [500, 600, 700, 800, 900, 950, 1000, 1100, 1200, 1300, 1400, 1500],  # ë¶€ìŠ¤íŒ… ë°˜ë³µ íšŸìˆ˜
                'subsample': [0.3, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 1.0],  # ë°°ê¹… ìƒ˜í”Œë§ ë¹„ìœ¨
                'criterion': ['friedman_mse', 'squared_error'],  # ë¶„í•  ê¸°ì¤€
                'max_depth': [3, 4, 5, 6, 7, 8, 9, 10, 12],  # íŠ¸ë¦¬ ê¹Šì´
                'min_samples_split': [2, 5, 10, 15, 20, 25, 30, 50],  # ë…¸ë“œ ë¶„í•  ìµœì†Œ ìƒ˜í”Œ ìˆ˜
                'min_samples_leaf': [1, 3, 5, 10, 15, 20, 30],  # ë¦¬í”„ ë…¸ë“œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
                'max_features': ['sqrt', 'log2', None, 0.5, 0.7, 0.9], # í”¼ì²˜ ì„ íƒ ë°©ì‹ (íŠ¸ë¦¬ë§ˆë‹¤ ì‚¬ìš©í•  ë³€ìˆ˜ ê°œìˆ˜ ê²°ì •)
                'alpha': [0.5, 0.6, 0.7, 0.75, 0.85, 0.9, 0.95],  # Huber & Quantile ì†ì‹¤ í•¨ìˆ˜ ì¡°ì • (Huber/Quantile ì‚¬ìš© ì‹œ)
                'random_state': [42],  # ì¬í˜„ì„±ì„ ìœ„í•œ ëœë¤ ì‹œë“œ
                "ccp_alpha": [0.0, 0.00001, 0.0001, 0.001, 0.01],
            }
        }

    def load_hyperparameter_grids_from_file(self, json_path):
        """
        ì§€ì •í•œ json íŒŒì¼ë¡œë¶€í„° hyperparameter_grids ê°’ì„ ë®ì–´ì“´ë‹¤.
        """
        if not os.path.exists(json_path):
            ensembleForecaster_logger.error(f"Hyperparameter grid file not found: {json_path}")
            return
        import json
        with open(json_path, 'r', encoding='utf-8') as f:
            for key, value in json.load(f).items():
                # keyì— 'Model'ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ, ì—†ìœ¼ë©´ 'Model'ì„ ë¶™ì—¬ì„œ key ìƒì„±
                if 'Model' in key:
                    new_key = key
                else:
                    new_key = f"{key}Model"
                if key in self.hyperparameter_grids:
                    self.hyperparameter_grids[new_key].update(value)
                else:
                    self.hyperparameter_grids[new_key] = value

# ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
defaultConfig = DefaultConfig()
