import os
import re
import numpy as np
import pandas as pd
import pickle
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score, KFold, TimeSeriesSplit
from utils.AI.ensemble_forecaster.config import ensembleForecaster_logger
from utils.AI.ensemble_forecaster.utilities import get_sunrise_sunset
from sklearn.feature_selection import VarianceThreshold
from statsmodels.stats.outliers_influence import variance_inflation_factor

def shuffle_weekly_blocks_keep_index(X: pd.DataFrame, y: pd.DataFrame, week_len: int = 168):
    """
    ì£¼ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ì…”í”Œí•˜ë˜, ì¸ë±ìŠ¤(DATETIME)ëŠ” ìœ ì§€í•œë‹¤.
    """
    assert len(X) == len(y)
    total_len = len(X)
    num_weeks = total_len // week_len

    # ì˜ë¦¬ëŠ” ì˜ì—­ë§Œ ì‚¬ìš©
    X = X.iloc[:num_weeks * week_len]
    y = y.iloc[:num_weeks * week_len]

    # ì¸ë±ìŠ¤ ìœ ì§€ìš©
    fixed_index = X.index.copy()

    # ì£¼ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸°
    X_weeks = [X.iloc[i * week_len : (i + 1) * week_len].reset_index(drop=True) for i in range(num_weeks)]
    y_weeks = [y.iloc[i * week_len : (i + 1) * week_len].reset_index(drop=True) for i in range(num_weeks)]

    # ì…”í”Œ ìˆœì„œ
    indices = np.arange(num_weeks)
    np.random.shuffle(indices)

    # ì…”í”Œ í›„ concat
    X_shuffled = pd.concat([X_weeks[i] for i in indices], ignore_index=True)
    y_shuffled = pd.concat([y_weeks[i] for i in indices], ignore_index=True)

    # ì¸ë±ìŠ¤ëŠ” ì›ë˜ ì‹œê°„ íë¦„ëŒ€ë¡œ ê³ ì •
    X_shuffled.index = fixed_index
    y_shuffled.index = fixed_index

    return X_shuffled, y_shuffled


def create_sequence(X, y, input_window=288, output_window=1, step_size=1):
    Xs, ys = [], []
    for i in range(0, len(X) - input_window - output_window + 1, step_size):
        v = X[i:(i + input_window)]
        label = y[(i + input_window):(i + input_window + output_window)]
        Xs.append(v)
        ys.append(label)
    return np.array(Xs), np.array(ys)

def filter_vif(X, thresh=5.0, max_iter=10):
    """
    ë°˜ë³µì ìœ¼ë¡œ VIF ê³„ì‚°í•˜ì—¬ ì„ê³„ê°’ ì´ˆê³¼ ë³€ìˆ˜ ì œê±°
    """
    cols = list(X.columns)
    for _ in range(max_iter):
        # Xë¥¼ floatìœ¼ë¡œ ë³€í™˜í•˜ì—¬ statsmodels í˜¸í™˜
        X_vals = X[cols].astype(float).values
        # RuntimeWarning ë¬´ì‹œ ë° infinite ì²˜ë¦¬
        import warnings
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            raw_vif = [variance_inflation_factor(X_vals, i) for i in range(X_vals.shape[1])]
        # NaN ë˜ëŠ” infëŠ” í° ê°’ìœ¼ë¡œ ëŒ€ì²´
        vif_vals = [float('inf') if (np.isnan(v) or np.isinf(v)) else v for v in raw_vif]
        max_vif = max(vif_vals)
        if max_vif <= thresh:
            break
        drop_idx = vif_vals.index(max_vif)
        cols.pop(drop_idx)
    return cols

def feature_filter_auto(
    X_train: pd.DataFrame,
    y_train: pd.Series = None,
    X_test: pd.DataFrame = None,
    variance_thresh: float = None,
    covariance_thresh: float = None,
    correlation_thresh: float = None,
    keep_ratio: float = 0.8,
    auto_tune: bool = True,
    use_cov_corr: bool = False,
    use_vif: bool = True,
    vif_thresh: float = 5.0,
    vif_max_iter: int = 10,
    return_report: bool = False,
):
    """
    ìë™ìœ¼ë¡œ í”¼ì²˜ë¥¼ í•„í„°ë§í•˜ëŠ” í•¨ìˆ˜.
    - ë¶„ì‚° ê¸°ì¤€, ê³µë¶„ì‚°/ìƒê´€ê³„ìˆ˜ ê¸°ì¤€, VIF ê¸°ì¤€ ë“±ì„ ì ìš©í•˜ì—¬ í”¼ì²˜ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
    
    Parameters:
    - X_train: pd.DataFrame
    - y_train: pd.Series or None (ê³µë¶„ì‚°/ìƒê´€ê³„ìˆ˜ ê¸°ì¤€ ì ìš© ì‹œ í•„ìš”)
    - X_test: pd.DataFrame or None
    - variance_thresh: float or None
    - covariance_thresh: float or None
    - correlation_thresh: float or None
    - keep_ratio: float (auto_tune=Trueì¼ ë•Œë§Œ ì‚¬ìš©)
    - auto_tune: bool
    - use_cov_corr: bool (Trueë©´ y_trainê³¼ì˜ ê³µë¶„ì‚°/ìƒê´€ê³„ìˆ˜ í•„í„°ë„ ì ìš©)
    - use_vif: bool (Trueë©´ VIF í•„í„°ë§ ì ìš©)
    - vif_thresh: float (VIF ì„ê³„ê°’)
    - vif_max_iter: int (VIF ë°˜ë³µ ìµœëŒ€ íšŸìˆ˜)

    Returns:
    - X_train_reduced: pd.DataFrame
    - X_test_reduced: pd.DataFrame or None
    - selected_columns: pd.Index
    """

    # ìë™ í”¼ì²˜ í•„í„°ë§ ë° ë¶„ì„ ë³´ê³ ì„œë¥¼ JSON í˜•íƒœë¡œ ìƒì„±í•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜.
    import json
    from scipy.stats import pearsonr
    # ë³´ê³ ìš© êµ¬ì¡° ì´ˆê¸°í™”
    report = {'cov_corr': [], 'regression': [], 'feature_importance': [], 'vif': []} if return_report else None

    # ë¶„ì‚° ê¸°ì¤€
    variances = X_train.var()

    # 1. auto-tune: ì„ê³„ê°’ ì—†ì„ ë•Œ ìë™ ì„ê³„ê°’ ì‚°ì¶œ
    def auto_thresh(series, keep_ratio):
        return series.quantile(1 - keep_ratio)

    vth = variance_thresh
    if auto_tune and vth is None:
        vth = auto_thresh(variances, keep_ratio)
    selected_mask = variances > vth

    # 2. (ì„ íƒ) ê³µë¶„ì‚°, ìƒê´€ê³„ìˆ˜ ì¶”ê°€ í•„í„°
    if use_cov_corr and y_train is not None:
        y_target_series = y_train
        if isinstance(y_train, pd.DataFrame):
            if y_train.shape[1] == 0:
                raise ValueError("feature_filter_auto: y_train is an empty DataFrame.")
            if y_train.shape[1] > 1:
                ensembleForecaster_logger.warning(
                    "feature_filter_auto: y_train is a DataFrame with {} columns. "
                    "Using the first column ('{}') for covariance/correlation calculation with features.".format(
                        y_train.shape[1], y_train.columns[0]
                    )
                )
            y_target_series = y_train.iloc[:, 0]
        elif not isinstance(y_train, pd.Series):
            try:
                temp_series = pd.Series(np.asarray(y_train).ravel())
                if len(temp_series) == X_train.shape[0]:
                    y_target_series = temp_series
                    ensembleForecaster_logger.warning(
                        "feature_filter_auto: y_train was not a pd.Series or pd.DataFrame but was converted to a pd.Series."
                    )
                else:
                    raise ValueError(
                        "feature_filter_auto: y_train is not a pd.Series or pd.DataFrame, and its length "
                        "does not match X_train after attempting conversion."
                    )
            except Exception as e:
                 raise ValueError(
                    f"feature_filter_auto: y_train is not a pd.Series or pd.DataFrame and could not be converted to a 1D Series. Error: {e}"
                )

        covariances = X_train.apply(lambda col: np.cov(col, y_target_series)[0, 1])
        
        # Helper function for safe correlation calculation
        def safe_corrcoef(x_series, y_series):
            if len(x_series) < 2 or len(y_series) < 2:
                return 0.0 # Correlation is undefined for less than 2 points

            x_std = x_series.std()
            y_std = y_series.std()

            # Check for NaN std or near-zero std (constant series)
            if pd.isna(x_std) or x_std < 1e-9 or \
               pd.isna(y_std) or y_std < 1e-9:
                return 0.0
            else:
                # Suppress RuntimeWarning locally as an additional precaution
                with np.errstate(divide='ignore', invalid='ignore'):
                    c = np.corrcoef(x_series, y_series)
                    # np.corrcoef returns a 2x2 matrix; we need the off-diagonal element.
                    # It can also return NaN if inputs are problematic despite std checks.
                    if c.shape == (2,2) and not np.isnan(c[0,1]):
                        return c[0,1]
                    else:
                        return 0.0

        correlations = X_train.apply(lambda col: safe_corrcoef(col, y_target_series))

        cth = covariance_thresh
        if auto_tune and cth is None:
            cth = covariances.abs().quantile(1 - keep_ratio)
        selected_mask &= covariances.abs() > cth

        rth = correlation_thresh
        if auto_tune and rth is None:
            rth = correlations.abs().quantile(1 - keep_ratio)
        selected_mask &= correlations.abs() > rth

    selected_columns = X_train.columns[selected_mask]
    # âœ… VIF í•„í„°ë§ ë° í•´ì„
    if use_vif and len(selected_columns) > 0:
        # VIF ê°’ ê³„ì‚°
        df_vif = X_train[selected_columns].astype(float)
        vif_matrix = df_vif.values
        vif_vals = [variance_inflation_factor(vif_matrix, j) for j in range(vif_matrix.shape[1])]
        new_cols = []
        for col, val in zip(selected_columns, vif_vals):
            # í•´ì„ ë° ë¹„ê³  ì„¤ì •
            if np.isnan(val):
                interp = 'ë°ì´í„° ì˜¤ë¥˜/ë¶„ì‚° 0 or ì™„ì „ ì¤‘ë³µ (ì¦‰ì‹œ ì œê±° ê¶Œê³ )'
            elif np.isinf(val):
                interp = 'ì ˆëŒ€ì  ë‹¤ì¤‘ê³µì„ ì„± or ì •ë³´ ì—†ìŒ (ì¦‰ì‹œ ì œê±° ê¶Œê³ )'
            elif val >= 10:
                interp = 'ì‹¬ê°í•œ ë‹¤ì¤‘ê³µì„ ì„± (ë³€ìˆ˜ ì œê±°/ì°¨ì› ì¶•ì†Œ ê¶Œê³ )'
            elif val >= 5:
                interp = 'ë‹¤ì¤‘ê³µì„ ì„± ê²½ê³„ì„  (ì‹ ì¤‘íˆ ê²€í† , ìœ ì§€ ê°€ëŠ¥)'
            else:
                interp = 'ë‹¤ì¤‘ê³µì„ ì„± ê±°ì˜ ì—†ìŒ (ì•ˆì „)'
            # ë³´ê³ ì„œ ê¸°ë¡
            if return_report:
                report['vif'].append({
                    'Feature': col,
                    'VIF': round(val, 2) if not np.isinf(val) and not np.isnan(val) else None,
                    'í•´ì„/ì¡°ì¹˜': interp
                })
            # í•„í„°ë§: NaN/inf/>=10 ì€ ì œì™¸
            if not (np.isnan(val) or np.isinf(val) or val >= 10):
                new_cols.append(col)
        selected_columns = pd.Index(new_cols)

    X_train_reduced = X_train[selected_columns]
    X_test_reduced = X_test[selected_columns] if X_test is not None else None
    # ë³´ê³ ìš© ìƒê´€/í”¼ì–´ìŠ¨ ë¶„ì„ ë° ì¤‘ìš”ë„
    if return_report and use_cov_corr and y_train is not None:
         y_series = y_train.iloc[:,0] if isinstance(y_train, pd.DataFrame) else pd.Series(y_train)
         n = len(X_train)
         for col in X_train.columns:
             try:
                 corr, p = pearsonr(X_train[col], y_series)
                 # t-value for Pearson correlation: t = r * sqrt((n-2)/(1-r^2))
                 t_val = corr * np.sqrt((n - 2) / (1 - corr**2)) if abs(corr) < 1 else np.inf
             except Exception:
                 corr, p, t_val = 0.0, None, None
             report['cov_corr'].append({
                 'Feature': col,
                 'Type': 'Numeric' if np.issubdtype(X_train[col].dtype, np.number) else 'Categorical',
                 'Corr.(target)': round(corr, 2),
                 't-value': round(t_val, 2) if t_val is not None and np.isfinite(t_val) else None,
                 'p-value': round(p, 4) if p is not None else None,
                 'í•´ì„/ë¹„ê³ ': ('ê°•í•œ' if abs(corr)>0.6 else 'ì¤‘ê°„' if abs(corr)>0.3 else 'ì•½í•œ') + ' ìƒê´€' if p and p<0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'
             })
    # íšŒê·€ ê³„ìˆ˜ ë¶„ì„: ë‹¨ë³€ëŸ‰ OLS
    if return_report and use_cov_corr and y_train is not None:
        import statsmodels.api as sm
        for col in X_train.columns:
            try:
                X_col = sm.add_constant(X_train[col])
                model = sm.OLS(y_series, X_col).fit()
                coef = model.params[col]
                se = model.bse[col]
                tval = model.tvalues[col]
                pval = model.pvalues[col]
                # Interpretation and recommendation
                sign = 'ì–‘ì˜' if coef > 0 else 'ìŒì˜'
                interp = f"ë¶€í˜¸({ '+' if coef>0 else '-' }): {sign} ë°©í–¥ ì˜í–¥, ì ˆëŒ€ê°’={abs(coef):.2f}"
                if pval < 0.05:
                    rec = 'p-value<0.05: ìœ ì˜, ë³€ìˆ˜ ìœ ì§€'
                elif pval < 0.1:
                    rec = 'p-value<0.1: ìœ ì˜ìˆ˜ì¤€ ë‚®ìŒ, ìœ ì§€ ê²€í† '
                else:
                    rec = 'p-value>=0.1: ì˜í–¥ ë¯¸ë¯¸, ì œê±°/í•´ì„ ì‹œ ì£¼ì˜'
            except Exception:
                coef, se, tval, pval, interp, rec = None, None, None, None, 'ë¶„ì„ ì‹¤íŒ¨', ''
            report['regression'].append({
                'Feature': col,
                'Coefficient': round(coef,2) if coef is not None else None,
                'Std.Err': round(se,2) if se is not None else None,
                't-value': round(tval,2) if tval is not None else None,
                'p-value': round(pval,4) if pval is not None else None,
                'í•´ì„': interp,
                'ì¡°ì¹˜ê¶Œê³ ': rec
            })

    if return_report:
        # Feature importance via RandomForest
        rf = RandomForestRegressor(random_state=42)
        rf.fit(X_train[selected_columns], y_series if use_cov_corr and y_train is not None else y_train)
        for col, imp in zip(selected_columns, rf.feature_importances_):
            report['feature_importance'].append({
                'Feature': col,
                'Importance': round(imp, 2),
                'ì„ ì •/ì œì™¸ ì‚¬ìœ ': 'ì¤‘ìš”' if imp > np.mean(rf.feature_importances_) else 'ì œì™¸'
            })

    # ë³´ê³ ì„œ ë°˜í™˜ ì—¬ë¶€
    if return_report:
        return X_train_reduced, X_test_reduced, selected_columns, report
    else:
        return X_train_reduced, X_test_reduced, selected_columns, None

def low_variance_feature_filter(X_train, X_test=None, threshold=0.01):
    """
    ë¶„ì‚°ì´ ë‚®ì€ í”¼ì²˜ë¥¼ ì œê±°í•˜ê³ , ë™ì¼í•œ í”¼ì²˜ë§Œ X_testì—ë„ ì ìš©.
    
    Parameters:
    - X_train: pd.DataFrame, í›ˆë ¨ìš© íŠ¹ì„± ë°ì´í„°
    - X_test: pd.DataFrame or None, í…ŒìŠ¤íŠ¸ìš© íŠ¹ì„± ë°ì´í„° (ê°™ì€ ì»¬ëŸ¼ êµ¬ì¡°ì—¬ì•¼ í•¨)
    - threshold: float, ë¶„ì‚° ì„ê³„ê°’

    Returns:
    - X_train_reduced, X_test_reduced, selected_columns
    """
    selector = VarianceThreshold(threshold=threshold)
    selector.fit(X_train)
    
    selected_mask = selector.get_support()
    selected_columns = X_train.columns[selected_mask]
    
    X_train_reduced = X_train[selected_columns]
    
    if X_test is not None:
        X_test_reduced = X_test[selected_columns]
    else:
        X_test_reduced = None

    return X_train_reduced, X_test_reduced, selected_columns

def classify_correlation_level(value):
    """ ìƒê´€ê´€ê³„ ê°•ë„ë¥¼ Levelë¡œ ë¶„ë¥˜í•˜ëŠ” í•¨ìˆ˜ """
    if value > 0.6:
        return "Strong"
    elif value > 0.3:
        return "Moderate"
    else:
        return "Weak"



def transform_new_data(df_new, scaler_dict):
    """
    ìƒˆë¡œìš´ ë°ì´í„°ì— ê¸°ì¡´ í•™ìŠµëœ Scalerë¥¼ ì ìš©í•˜ëŠ” í•¨ìˆ˜.

    Parameters:
        df_new (pd.DataFrame): ë³€í™˜í•  ìƒˆë¡œìš´ ë°ì´í„°í”„ë ˆì„
        scaler_dict (dict): ê¸°ì¡´ í•™ìŠµëœ Scaler ê°ì²´ ë”•ì…”ë„ˆë¦¬

    Returns:
        pd.DataFrame: ë³€í™˜ëœ ë°ì´í„°í”„ë ˆì„
    """
    transformed_df = df_new.copy()
    for col in scaler_dict:
        scaler = scaler_dict[col]
        transformed_df[col] = scaler.transform(df_new[[col]])
    else:
        print(f"âš ï¸ Warning: No scaler found for {col}. Skipping transformation.")

    return transformed_df


def save_scalers(scaler_dict, base_dir="scalers", trial_num=None):
    """
    Scaler ê°ì²´ë¥¼ 'scalers/trial_ìˆ«ì/scalers.pkl' í˜•ì‹ìœ¼ë¡œ ì €ì¥

    Parameters:
        scaler_dict (dict): í•™ìŠµëœ Scaler ê°ì²´ê°€ ì €ì¥ëœ ë”•ì…”ë„ˆë¦¬
        base_dir (str): ì €ì¥í•  ê¸°ë³¸ ë””ë ‰í† ë¦¬
        trial_num (int or None): trial ë²ˆí˜¸ ì§€ì • (Noneì´ë©´ ìë™ ì¦ê°€)
    """
    os.makedirs(base_dir, exist_ok=True)

    if trial_num is None:
        # ìë™ trial ë²ˆí˜¸ ê³„ì‚°
        existing_trials = [
            int(match.group(1)) for d in os.listdir(base_dir)
            if (match := re.match(r"trial_(\d+)", d)) and os.path.isdir(os.path.join(base_dir, d))
        ]
        trial_num = max(existing_trials, default=0) + 1

    trial_path = os.path.join(base_dir, f"trial_{trial_num}")
    os.makedirs(trial_path, exist_ok=True)

    file_path = os.path.join(trial_path, "scalers.pkl")
    with open(file_path, "wb") as f:
        pickle.dump(scaler_dict, f)


def load_scalers(base_dir="scalers", trial_num=None):
    """
    ê°€ì¥ ìµœê·¼ ë˜ëŠ” ì§€ì •ëœ trial ë””ë ‰í† ë¦¬ì—ì„œ scalers.pkl íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°

    Parameters:
        base_dir (str): ê¸°ë³¸ ë””ë ‰í† ë¦¬
        trial_num (int or None): íŠ¹ì • trial ë²ˆí˜¸ (Noneì´ë©´ ìµœì‹  ìë™ ì„ íƒ)

    Returns:
        dict: Scaler ê°ì²´ ë”•ì…”ë„ˆë¦¬
    """
    if not os.path.exists(base_dir):
        raise FileNotFoundError(f"{base_dir} ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    if trial_num is not None:
        trial_path = os.path.join(base_dir, f"trial_{trial_num}")
        if not os.path.exists(trial_path):
            raise FileNotFoundError(f"{trial_path} ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    else:
        # ê°€ì¥ ìµœì‹  trial ìë™ ì„ íƒ
        existing_trials = [
            (int(match.group(1)), os.path.join(base_dir, d)) for d in os.listdir(base_dir)
            if (match := re.match(r"trial_(\d+)", d)) and os.path.isdir(os.path.join(base_dir, d))
        ]
        if not existing_trials:
            raise FileNotFoundError("ì €ì¥ëœ trial_í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
        trial_path = max(existing_trials, key=lambda x: x[0])[1]

    file_path = os.path.join(trial_path, "scalers.pkl")
    with open(file_path, "rb") as f:
        scaler_dict = pickle.load(f)
    return scaler_dict


def feature_optimizer_with_cv(X_train, y_train, model, method="pre", detection_results=None, n_splits=5, scoring="neg_mean_absolute_error"):
    """
    íŠ¹ì§• ì„ íƒ ìµœì í™” í•¨ìˆ˜ (íŠœë‹ ì „/í›„ ì ìš© ê°€ëŠ¥) + êµì°¨ ê²€ì¦ ê¸°ëŠ¥ ì¶”ê°€ + ì‹œê³„ì—´ ì—¬ë¶€ ê°ì§€

    Parameters:
        X_train (DataFrame): ì…ë ¥ íŠ¹ì„± ë°ì´í„°
        y_train (Series): íƒ€ê²Ÿ ë³€ìˆ˜
        model (sklearn model): ì‚¬ìš©í•  ëª¨ë¸ (íŠœë‹ í›„ íŠ¹ì§• ì„ íƒ ì‹œ í•„ìš”)
        method (str): "pre" - íŠœë‹ ì „, "post" - íŠœë‹ í›„
        detection_results (dict): ì‹œê³„ì—´ ë°ì´í„° ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” ì‚¬ì „(dict), ì˜ˆ: {"is_sequential": True}
        n_splits (int): êµì°¨ ê²€ì¦ í´ë“œ ìˆ˜ (ê¸°ë³¸ê°’: 5)
        scoring (str): sklearn í‰ê°€ ì§€í‘œ (ê¸°ë³¸ê°’: "neg_mean_absolute_error")

    Returns:
        selected_features (list): ì„ íƒëœ ìµœì  íŠ¹ì„± ë¦¬ìŠ¤íŠ¸
    """

    # ğŸš€ êµì°¨ ê²€ì¦ ë°©ì‹ ìë™ ì„ íƒ
    if detection_results is not None and detection_results.get("is_sequential", False):
        spliter = TimeSeriesSplit(n_splits=n_splits) 
    else: 
        spliter = KFold(n_splits=n_splits, shuffle=True)

    if method == "pre":
        # 1ï¸âƒ£ ë‹¤ì¤‘ê³µì„ ì„± ê¸°ë°˜ íŠ¹ì§• ì œê±°
        correlation_matrix = X_train.corr().abs()
        upper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))
        high_correlation_features = [column for column in upper.columns if any(upper[column] > 0.85)]
        X_train = X_train.drop(columns=high_correlation_features, errors='ignore')

        # 2ï¸âƒ£ RFE ê¸°ë°˜ íŠ¹ì§• ì œê±° (RandomForest ì‚¬ìš©)
        selector = RFE(RandomForestRegressor(n_estimators=400, random_state=42), n_features_to_select=5)
        selector.fit(X_train, y_train)
        selected_features = X_train.columns[selector.support_]

        # 3ï¸âƒ£ ì„ íƒëœ íŠ¹ì§•ìœ¼ë¡œ êµì°¨ ê²€ì¦ ìˆ˜í–‰
        scores = cross_val_score(model, X_train[selected_features], y_train, cv=spliter, scoring=scoring)
        print(f"Pre-Tuning CV Score: {np.mean(scores):.4f} Â± {np.std(scores):.4f}")

        # 4ï¸âƒ£ ì¶”ê°€ì ì¸ í‰ê°€ ì§€í‘œ(RÂ², MSE, MAE, MAPE, CV(RMSE), NMBE) ë¶„ì„
        r2_scores, mse_scores, mae_scores, mape_scores, cvrmse_scores, nmbe_scores = [], [], [], [], [], []
        
        for train_idx, test_idx in spliter.split(X_train):
            X_train_fold, X_test_fold = X_train.iloc[train_idx][selected_features], X_train.iloc[test_idx][selected_features]
            y_train_fold, y_test_fold = y_train.iloc[train_idx], y_train.iloc[test_idx]

            model.fit(X_train_fold, y_train_fold)
            y_pred = model.predict(X_test_fold)

            # í‰ê°€ ì§€í‘œ ê³„ì‚°
            r2_scores.append(r2_score(y_test_fold, y_pred))
            mse_scores.append(mean_squared_error(y_test_fold, y_pred))
            mae_scores.append(mean_absolute_error(y_test_fold, y_pred))
            mape_scores.append(np.mean(np.abs((y_test_fold - y_pred) / y_test_fold)) * 100)
            cvrmse_scores.append(np.sqrt(np.mean((y_test_fold - y_pred) ** 2)) / np.mean(y_test_fold) * 100)
            nmbe_scores.append(np.mean(y_test_fold - y_pred) / np.mean(y_test_fold) * 100)
            
            print(f"RÂ² Score: {np.mean(r2_scores):.4f} Â± {np.std(r2_scores):.4f}")
            print(f"MSE: {np.mean(mse_scores):.4f} Â± {np.std(mse_scores):.4f}")
            print(f"MAE: {np.mean(mae_scores):.4f} Â± {np.std(mae_scores):.4f}")
            print(f"MAPE: {np.mean(mape_scores):.4f} Â± {np.std(mape_scores):.4f}")
            print(f"CV(RMSE): {np.mean(cvrmse_scores):.4f} Â± {np.std(cvrmse_scores):.4f}")
            print(f"NMBE: {np.mean(nmbe_scores):.4f} Â± {np.std(nmbe_scores):.4f}")

            # 5ï¸âƒ£ íŠ¹ì • ê¸°ì¤€ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ ë³€ìˆ˜ ì„ íƒ
            # ì˜ˆ: RÂ²ê°€ ë†’ê³ , MAPEì™€ CV(RMSE)ê°€ ë‚®ì€ ë³€ìˆ˜ ìœ ì§€
            if np.mean(mape_scores) > 20 or np.mean(cvrmse_scores) > 15:
                print("ğŸš¨ ë†’ì€ MAPE ë˜ëŠ” CV(RMSE) â†’ ì¶”ê°€ì ì¸ íŠ¹ì§• ì œê±° í•„ìš”")
                selected_features = selected_features[:-1]  # ì¤‘ìš”ë„ê°€ ë‚®ì€ ë³€ìˆ˜ ì¶”ê°€ ì œê±°
                
    elif method == "post" and model is not None:
        pass
    else:
        raise ValueError("Invalid method. Choose 'pre' for pre-tuning optimization or 'post' for post-tuning optimization.")

    return selected_features


def generate_sun_based_cyclic_features(df, lat, lon, mode="day_only"):
    """
    ì¼ì¶œ/ì¼ëª° ì‹œê°„ì— ë§ì¶° ì‚¬ì¸/ì½”ì‚¬ì¸ ë³€í™˜ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜.
    
    :param df: ì›ë³¸ ë°ì´í„°í”„ë ˆì„ (READ_DATETIMEì´ ì¸ë±ìŠ¤ë¡œ ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•¨)
    :param lat: ìœ„ë„
    :param lon: ê²½ë„
    :param mode: 'day_only', 'night_only', 'both' ì¤‘ ì„ íƒ (ê¸°ë³¸ê°’: 'both')
    :return: (ì¼ì¶œ/ì¼ëª° ê¸°ë°˜ ì‚¬ì¸/ì½”ì‚¬ì¸ íŠ¹ì§•ì´ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„, ì¶”ê°€ëœ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸)
    """
    if mode not in ["day_only", "night_only", "both"]:
        raise ValueError("mode ê°’ì€ 'day_only', 'night_only', 'both' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")

    df_expanded = df.copy()

    # âœ… indexì˜ ì‹œê°„ëŒ€ë¥¼ 'Asia/Seoul'ë¡œ ì„¤ì • (tz-naive â†’ tz-aware)
    if df_expanded.index.tz is None:
        df_expanded.index = df_expanded.index.tz_localize("Asia/Seoul")
    else:
        df_expanded.index = df_expanded.index.tz_convert("Asia/Seoul")

    # âœ… ë‚ ì§œë³„ ì¼ì¶œ/ì¼ëª° ì‹œê°„ ê³„ì‚°
    df_expanded["sunrise"] = [get_sunrise_sunset(lat, lon, date)[0] for date in df_expanded.index.date]
    df_expanded["sunset"] = [get_sunrise_sunset(lat, lon, date)[1] for date in df_expanded.index.date]

    # âœ… ë‚®(ì¼ì¶œ~ì¼ëª°)ì¸ì§€ ì—¬ë¶€ (tz-aware ë°ì´í„° ë¹„êµ ê°€ëŠ¥)
    df_expanded["is_daytime"] = ((df_expanded.index >= df_expanded["sunrise"]) & 
                                  (df_expanded.index <= df_expanded["sunset"]))

    # âœ… ë‚®/ë°¤ ì‹œê°„ì„ ì¼ì¶œ-ì¼ëª° ì£¼ê¸°ì— ë§ì¶° ì •ê·œí™”
    def normalize_time(timestamp, sunrise, sunset):
        if sunrise <= timestamp <= sunset:
            # ë‚® ì‹œê°„ ì •ê·œí™” (0 ~ Ï€)
            return np.pi * (timestamp - sunrise).total_seconds() / (sunset - sunrise).total_seconds()
        else:
            # ë°¤ ì‹œê°„ ì •ê·œí™” (Ï€ ~ 2Ï€)
            next_sunrise = get_sunrise_sunset(lat, lon, (timestamp + pd.Timedelta(days=1)).date())[0]
            return np.pi + np.pi * (timestamp - sunset).total_seconds() / (next_sunrise - sunset).total_seconds()

    df_expanded["normalized_time"] = [normalize_time(ts, sr, ss) for ts, sr, ss in 
                                      zip(df_expanded.index, df_expanded["sunrise"], df_expanded["sunset"])]

    # âœ… ì„ íƒí•œ modeì— ë”°ë¼ ì‚¬ì¸/ì½”ì‚¬ì¸ ë³€í™˜
    added_columns = ["is_daytime"]
    
    if mode == "day_only":
        df_expanded["sun_sin"] = np.where(df_expanded["is_daytime"], np.sin(df_expanded["normalized_time"]), 0)
        added_columns.append("sun_sin")

    elif mode == "night_only":
        df_expanded["sun_sin"] = np.where(~df_expanded["is_daytime"], np.sin(df_expanded["normalized_time"]), 0)
        added_columns.append("sun_sin")

    elif mode == "both":
        df_expanded["sun_sin"] = np.sin(df_expanded["normalized_time"])
        df_expanded["sun_cos"] = np.cos(df_expanded["normalized_time"])
        added_columns.extend(["sun_sin", "sun_cos"])

    # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°
    df_expanded.drop(columns=["sunrise", "sunset", "normalized_time"], inplace=True)

    ensembleForecaster_logger.info(f"ì¼ì¶œ/ì¼ëª° ì‹œê°„ì— ë§ì¶° ì‚¬ì¸/ì½”ì‚¬ì¸ ë³€í™˜ ìƒì„± : {added_columns}")
    
    nan_columns = df_expanded.columns[df_expanded.isna().all()].tolist()
    ensembleForecaster_logger.debug(f"NaNë§Œ í¬í•¨ëœ ì»¬ëŸ¼: {nan_columns}")
    
    return df_expanded, added_columns

def add_peak_features(raw_data, target_columns, detection_results, ref_interval=5, peak_threshold=2.0):
    """
    í”¼í¬ ê°ì§€ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ì¶”ê°€ì ì¸ í”¼í¬ ê´€ë ¨ ë³€ìˆ˜ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.

    Parameters:
        raw_data (pd.DataFrame): ì›ë³¸ ë°ì´í„°í”„ë ˆì„
        target (str): í”¼í¬ ê°ì§€ë¥¼ ìˆ˜í–‰í•  ëŒ€ìƒ ì»¬ëŸ¼ëª…
        detection_results (dict): íƒì§€ëœ íŒ¨í„´ ê²°ê³¼
        ref_interval (int): ë°ì´í„°ì˜ ê¸°ì¤€ ì‹œê°„ ê°„ê²© (ë¶„ ë‹¨ìœ„, ê¸°ë³¸ê°’ 5ë¶„)

    Returns:
        pd.DataFrame: ë³€í™˜ëœ ë°ì´í„°í”„ë ˆì„ (ìƒˆë¡œìš´ í”¼í¬ ê´€ë ¨ ë³€ìˆ˜ í¬í•¨)
        list: ì¶”ê°€ëœ íŠ¹ì§• ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
    """
    
    df_expanded = raw_data.copy()
    added_features = []

    for target in target_columns:
        # âœ… íƒì§€ëœ í”¼í¬ ì—¬ë¶€ í™•ì¸
        has_multiscale_features = detection_results[target].get("has_multiscale_features", False)
        has_peak_features = detection_results[target].get("has_peak_features", False)

        # âœ… í”¼í¬ê°€ ê°ì§€ëœ ê²½ìš°ì—ë§Œ í”¼ì²˜ ìƒì„±
        if has_peak_features or has_multiscale_features:
            
            # âœ… detection_resultsì— ì €ì¥ëœ í”¼í¬ ì¸ë±ìŠ¤ë¥¼ í™œìš©í•œ ë§ˆìŠ¤í¬ ìƒì„±
            peak_indices = detection_results[target].get("peak_indices", [])
            peak_ind = df_expanded.index.to_series().isin(peak_indices).astype(int)
            df_expanded[f"{target}_peak_indicator"] = peak_ind
            added_features.append(f"{target}_peak_indicator")
             # window definitions
            windows = {
                "ultra_short": int(0.5 * (60 / ref_interval)),
                "short": int(1 * (60 / ref_interval)),
                "long": int(3 * (60 / ref_interval))
            }
            for name, w in windows.items():
                # intensity mean, count, rate change, duration
                df_expanded[f"{target}_peak_intensity_mean_{name}"] = df_expanded[target].where(peak_ind == 1).rolling(window=w, min_periods=1).mean()
                df_expanded[f"{target}_peak_count_{name}"] = peak_ind.rolling(window=w, min_periods=1).sum().astype(int)
                df_expanded[f"{target}_peak_rate_change_{name}"] = df_expanded[f"{target}_peak_count_{name}"].diff().fillna(0).astype(int)
                df_expanded[f"{target}_peak_duration_{name}"] = peak_ind.groupby((peak_ind != peak_ind.shift()).cumsum()).cumsum()
                added_features += [
                     f"{target}_peak_intensity_mean_{name}", f"{target}_peak_count_{name}",
                     f"{target}_peak_rate_change_{name}", f"{target}_peak_duration_{name}"
                 ]
            df_expanded[f"{target}_cumulative_peak_count"] = peak_ind.cumsum().astype(int)
            df_expanded[f"{target}_time_since_last_peak"] = peak_ind.cumsum().diff().fillna(0).astype(int)
            added_features += [f"{target}_cumulative_peak_count", f"{target}_time_since_last_peak"]
            # peak trend from ultra-short intensity
            df_expanded[f"{target}_peak_trend"] = df_expanded[f"{target}_peak_intensity_mean_ultra_short"].diff().fillna(0).apply(lambda x: 1 if x>0 else 0)
            added_features.append(f"{target}_peak_trend")

    ensembleForecaster_logger.info(f"í”¼í¬ ê´€ë ¨ ë³€ìˆ˜ë¥¼ ìƒì„± : {added_features}")
    
    nan_columns = df_expanded.columns[df_expanded.isna().all()].tolist()
    ensembleForecaster_logger.debug(f"NaNë§Œ í¬í•¨ëœ ì»¬ëŸ¼: {nan_columns}")
    
    return df_expanded, added_features


# ì¾Œì ì§€ìˆ˜ ê³„ì‚° í•¨ìˆ˜
def discomfort_index(temp, hum):
    """
    ì˜¨ë„ì™€ ìŠµë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¾Œì ì§€ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

    ì¾Œì ì§€ìˆ˜ëŠ” ì—´ê³¼ ìŠµë„ë¡œ ì¸í•´ ì¸ê°„ì´ ê²½í—˜í•˜ëŠ” ë¶ˆì¾Œê°ì„ ì¶”ì •í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ì§€í‘œì…ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
    temp (float): ì„­ì”¨ ì˜¨ë„.
    hum (float): ìƒëŒ€ ìŠµë„ (0ì—ì„œ 100 ì‚¬ì´ì˜ ë°±ë¶„ìœ¨).

    ë°˜í™˜ê°’:
    float: ê³„ì‚°ëœ ì¾Œì ì§€ìˆ˜.
    """
    return 0.81 * temp + 0.01 * hum * (0.99 * temp - 14.3) + 46.3

def mapping_sensor_variables(raw_data):
    """
    ì„¼ì„œ ë³€ìˆ˜ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ìë™ìœ¼ë¡œ ì‹¤ë‚´/ì™¸, ì„¼ì„œ ì¢…ë¥˜(ì˜¨ë„/ìŠµë„/CO2)ë¡œ ë§µí•‘í•˜ëŠ” í•¨ìˆ˜

    Parameters:
        var_dict (dict): {ë³€ìˆ˜ëª…: íƒœê·¸ê°’} í˜•ì‹ì˜ ë”•ì…”ë„ˆë¦¬

    Returns:
        list of dict: ìë™ ë¶„ë¥˜ëœ ì„¼ì„œ ì •ë³´ ë¦¬ìŠ¤íŠ¸
    """
