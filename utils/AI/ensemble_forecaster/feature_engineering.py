from statsmodels.tsa.stattools import pacf
import numpy as np
import warnings

# dataëŠ” ì‹œê³„ì—´ ë°ì´í„° (numpy array ë˜ëŠ” pandas Series)
# requested_nlagsëŠ” ì‚¬ìš©ìê°€ ìš”ì²­í•œ nlags ê°’
def calculate_pacf_safely(data, requested_nlags):
    n_samples = len(data)
    
    # ìƒ˜í”Œ í¬ê¸°ì˜ 50%ë¥¼ ìµœëŒ€ nlagsë¡œ ì„¤ì • (ë‹¨, ìµœì†Œ 1 ì´ìƒ)
    # ì¼ë°˜ì ìœ¼ë¡œ pacf í•¨ìˆ˜ ìì²´ì—ì„œ nlagsì˜ ê¸°ë³¸ ìµœëŒ€ê°’ì„ n_samples // 2 - 1 ë“±ìœ¼ë¡œ ì œí•œí•˜ê¸°ë„ í•©ë‹ˆë‹¤.
    # statsmodelsì˜ pacfëŠ” ê¸°ë³¸ì ìœ¼ë¡œ min(n_samples // 2 - 1, 40) ì •ë„ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜, ì‚¬ìš©ìê°€ ì§€ì •í•œ nlagsë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    # ì—¬ê¸°ì„œëŠ” ëª…ì‹œì ìœ¼ë¡œ 50% ì œí•œì„ ë‘ê² ìŠµë‹ˆë‹¤.
    max_permissible_nlags = max(1, n_samples // 2 -1) # -1ì€ ë³´ìˆ˜ì ì¸ ì ‘ê·¼

    actual_nlags = requested_nlags
    
    if requested_nlags >= max_permissible_nlags:
        warnings.warn(
            f"Requested nlags {requested_nlags} is too large for sample size {n_samples}. "
            f"Adjusting nlags to {max_permissible_nlags}.",
            UserWarning
        )
        actual_nlags = max_permissible_nlags
        
    if actual_nlags <= 0 and n_samples > 1 : # ìµœì†Œí•œì˜ lagëŠ” 1ì´ ë˜ë„ë¡ (ìƒ˜í”Œì´ 2ê°œ ì´ìƒì¼ë•Œ)
        actual_nlags = 1
    elif n_samples <=1: # ìƒ˜í”Œì´ ë„ˆë¬´ ì ì–´ PACF ê³„ì‚° ë¶ˆê°€
        warnings.warn(
            f"Sample size {n_samples} is too small to compute PACF.",
            UserWarning
        )
        return np.array([]) # ë¹ˆ ë°°ì—´ ë˜ëŠ” ì ì ˆí•œ ê°’ ë°˜í™˜

    try:
        # method='ywm' ë˜ëŠ” 'ols' ë“± ìƒí™©ì— ë§ëŠ” ë©”ì„œë“œ ì‚¬ìš©
        pacf_values = pacf(data, nlags=actual_nlags, method='ywm') 
        return pacf_values
    except ValueError as e:
        warnings.warn(f"Error computing PACF even after adjusting nlags: {e}", UserWarning)
        return np.array([]) # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë°°ì—´ ë˜ëŠ” ì ì ˆí•œ ê°’ ë°˜í™˜

# ì‚¬ìš© ì˜ˆì‹œ
# some_time_series_data = [...] 
# user_defined_nlags = 30
# pacf_results = calculate_pacf_safely(some_time_series_data, user_defined_nlags)
# if pacf_results.size > 0:
#     # PACF ê²°ê³¼ ì‚¬ìš©
# else:
#     # PACF ê³„ì‚° ì‹¤íŒ¨ ì²˜ë¦¬

import numpy as np
import holidays  # ëŒ€í•œë¯¼êµ­ì˜ ê³µíœ´ì¼ ì •ë³´ë¥¼ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€
import pandas as pd
from statsmodels.tsa.stattools import adfuller, acf, pacf
from scipy.stats import skew, kurtosis, shapiro, zscore
from scipy.stats.mstats import winsorize
from sklearn.cluster import DBSCAN
from sklearn.neighbors import LocalOutlierFactor
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler, QuantileTransformer, PowerTransformer
from utils.AI.ensemble_forecaster.config import ensembleForecaster_logger

import numpy as np
from scipy.stats import shapiro, anderson, normaltest

def get_normality_pvalue(data: np.ndarray) -> float:
    data = data.flatten()
    N = len(data)

    if N <= 50:
        _, p = shapiro(data)
    elif N <= 300:
        result = anderson(data)
        # Andersonì€ p-valueë¥¼ ë°˜í™˜í•˜ì§€ ì•ŠìŒ, ì„ê³„ê°’ ê¸°ë°˜ ì¶”ì • (ì„ì˜ ê¸°ì¤€ 5% ì‚¬ìš©)
        p = 0.05 if result.statistic > result.critical_values[2] else 0.1
    else:
        _, p = normaltest(data)

    return p

def classify_correlation_level(value):
    """ ìƒê´€ê´€ê³„ ê°•ë„ë¥¼ Levelë¡œ ë¶„ë¥˜í•˜ëŠ” í•¨ìˆ˜ """
    if value > 0.6:
        return "Strong"
    elif value > 0.3:
        return "Moderate"
    else:
        return "Weak"


def detect_time_series_patterns(
    raw_data: pd.DataFrame,
    target_columns=None,
    max_lag=30,  # 288 = 24ì‹œê°„ * 12 (5ë¶„ ê°„ê²©)
    ref_interval=5,
    peak_threshold=2.0,
):
    """
    ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ ë‹¤ì–‘í•œ íŒ¨í„´(ì •ë ¬, ì‹œê°„ ì˜ì¡´ì„±, ì£¼ê¸°ì„±, ë…¸ì´ì¦ˆ, ì´ìƒì¹˜, ê²°ì¸¡ì¹˜, íŠ¸ë Œë“œ, ë³€í™” ë“±)ì„ íƒì§€í•˜ëŠ” í•¨ìˆ˜.
    Lag(ì‹œì°¨) featureê°€ í•„ìš”í•œì§€ì™€ ì ì ˆí•œ lag ì¶”ì²œê¹Œì§€ í¬í•¨.
    ë‹¤ì¤‘ ì»¬ëŸ¼ ë™ì‹œ ì§€ì›.
    ë¡œê·¸ëŠ” ensembleForecaster_loggerì— ê¸°ë¡ë¨.
    """

    # === ë¡œê·¸ í•¨ìˆ˜: ensembleForecaster_loggerë¡œ ê³ ì • ===
    def log(msg, level='info'):
        import builtins
        if 'ensembleForecaster_logger' in globals():
            logger = globals()['ensembleForecaster_logger']
        else:
            # ì—†ëŠ” ê²½ìš° print fallback (ì£¼ë¡œ í…ŒìŠ¤íŠ¸/ë””ë²„ê¹…ìš©)
            logger = None
        if logger:
            if hasattr(logger, level):
                getattr(logger, level)(msg)
            else:
                logger.info(msg)
        else:
            builtins.print(msg)

    if target_columns is None:
        target_columns = raw_data.select_dtypes(include=[np.number]).columns.tolist()

    detection_results = {}

    # 1. ì‹œê³„ì—´ ë°ì´í„° ì •ë ¬ ì—¬ë¶€ (ìˆœì°¨ì„±)
    detection_results["is_sequential"] = raw_data.index.is_monotonic_increasing
    log(f"ë°ì´í„°ê°€ ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì •ë ¬ë¨: {detection_results['is_sequential']}")

    # 2. ì‹œê°„ ì˜ì¡´ì„±
    has_temporal_dependency = (raw_data.index.to_series().diff().dt.total_seconds().fillna(0) > 0).any()
    detection_results["has_temporal_dependency"] = has_temporal_dependency
    log(f"ë°ì´í„°ê°€ ì‹œê°„ íë¦„ì— ì˜ì¡´í•¨: {has_temporal_dependency}")

    # 3. ê²°ì¸¡ì¹˜ íƒì§€
    detection_results["has_missing_values"] = raw_data.isnull().any().any()
    log(f"ë°ì´í„°ì— ê²°ì¸¡ì¹˜ ì¡´ì¬ ì—¬ë¶€: {detection_results['has_missing_values']}")

    for target in target_columns:
        detection_results[target] = {}

        # 4. ë…¸ì´ì¦ˆ(ì´ìƒì¹˜) íƒì§€
        try:
            window_size = max(1, int(60 / ref_interval))
            rolling_mean = raw_data[target].rolling(window=window_size, min_periods=1).mean()
            noise_mask = np.abs(raw_data[target] - rolling_mean) > rolling_mean.std()
            detection_results[target]["is_noisy"] = noise_mask.any()
            log(f"{target} ë°ì´í„°ì— ë…¸ì´ì¦ˆ(ì´ìƒì¹˜) ì¡´ì¬ ì—¬ë¶€: {detection_results[target]['is_noisy']}")
        except Exception as err:
            detection_results[target]["is_noisy"] = False
            log(f"{target} ì»¬ëŸ¼ì´ ì—†ì–´ ë…¸ì´ì¦ˆ ê°ì§€ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. {err}", level="warning")

        # 5. íŠ¸ë Œë“œ ë° í•­ìƒ ì¦ê°€ ì—¬ë¶€
        try:
            detection_results[target]["has_trend"] = raw_data[target].diff().fillna(0).cumsum().iloc[-1] > 0
            detection_results[target]["is_always_increasing"] = raw_data[target].diff().dropna().ge(0).all()
            log(f"{target} ë°ì´í„°ê°€ íŠ¸ë Œë“œë¥¼ ê°€ì§€ëŠ”ì§€ í™•ì¸: {detection_results[target]['has_trend']}")
            log(f"{target} ë°ì´í„°ê°€ í•­ìƒ ì¦ê°€í•˜ëŠ” í˜•íƒœì¸ì§€ í™•ì¸: {detection_results[target]['is_always_increasing']}")
        except Exception as err:
            detection_results[target]["has_trend"] = False
            detection_results[target]["is_always_increasing"] = False
            log(f"{target} ì»¬ëŸ¼ì´ ì—†ì–´ íŠ¸ë Œë“œ ê°ì§€ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. {err}", level="warning")

        # 6. ë‹¨ê¸°ì /ì¥ê¸°ì  ë³€í™”, í”¼í¬(ê¸‰ê²©ë³€ë™) íƒì§€
        try:
            ultra_short_window = int(0.5 * (60 / ref_interval))
            short_window = int(1 * (60 / ref_interval))
            long_window = int(3 * (60 / ref_interval))

            ultra_short_term_trend = raw_data[target].diff().abs().rolling(window=ultra_short_window, min_periods=1).mean()
            short_term_trend = raw_data[target].diff().abs().rolling(window=short_window, min_periods=1).mean()
            long_term_trend = raw_data[target].diff().abs().rolling(window=long_window, min_periods=1).mean()

            detection_results[target]["has_multiscale_features"] = (
                (ultra_short_term_trend > 0).any() and
                (short_term_trend > 0).any() and
                (long_term_trend > 0).any()
            )

            value_diff = raw_data[target].diff().fillna(0)
            value_pct_change = raw_data[target].pct_change().fillna(0)
            rolling_mean = raw_data[target].rolling(window=short_window, min_periods=1).mean()
            rolling_std = raw_data[target].rolling(window=short_window, min_periods=1).std().fillna(0)

            # í”¼í¬(ê¸‰ê²©ë³€ë™) íƒì§€
            peak_indicator = (
                (value_diff.abs() > rolling_std * peak_threshold) |
                (value_pct_change.abs() > rolling_std * peak_threshold) |
                (ultra_short_term_trend > rolling_std * peak_threshold)
            ).astype(int)

            detection_results[target]["has_peak_features"] = int(peak_indicator.sum()) > 0
            detection_results[target]["peak_count"] = int(peak_indicator.sum())
            detection_results[target]["peak_indices"] = list(raw_data.index[peak_indicator == 1])

            log(f"{target}: í”¼í¬(ê¸‰ê²©ë³€ë™) ê°œìˆ˜: {detection_results[target]['peak_count']}")
            log(f"{target}: ì´ˆë‹¨ê¸°, ë‹¨ê¸°, ì¥ê¸° ë³€í™”ëŸ‰ ê¸°ë°˜ í”¼í¬ ê°ì§€ ê²°ê³¼: {detection_results[target]['has_peak_features']}")

        except Exception as err:
            detection_results[target]["has_multiscale_features"] = False
            detection_results[target]["has_peak_features"] = False
            detection_results[target]["peak_count"] = 0
            detection_results[target]["peak_indices"] = []
            log(f"{target} ì»¬ëŸ¼ì´ ì—†ì–´ ë‹¨ê¸°/ì¥ê¸° ë³€í™” ë˜ëŠ” í”¼í¬ ê°ì§€ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. {err}", level="warning")

        # 7. Lag(ì‹œì°¨) ê´€ë ¨ Feature í•„ìš”ì„± ë° ì¶”ì²œ (ADF ì œê±°ë¨)
        target_data_dropna = raw_data[target].dropna()
        n_samples_acf = len(target_data_dropna)
        actual_nlags_acf = min(max_lag, n_samples_acf - 1) if n_samples_acf > 1 else 0
        if actual_nlags_acf > 0:
            acf_values = acf(target_data_dropna, nlags=actual_nlags_acf)
        else:
            acf_values = np.array([])
            log(f"[{target}] ìƒ˜í”Œ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ì•„ ACFë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {n_samples_acf}", level="warning")

        n_samples_pacf = len(target_data_dropna)
        max_permissible_nlags_pacf = max(1, n_samples_pacf // 2 - 1) if n_samples_pacf > 3 else 0
        actual_nlags_pacf = max_lag
        if max_lag >= max_permissible_nlags_pacf and max_permissible_nlags_pacf > 0:
            warnings.warn(
                f"Requested nlags {max_lag} for PACF is too large for sample size {n_samples_pacf}. "
                f"Adjusting nlags to {max_permissible_nlags_pacf}.",
                UserWarning
            )
            log(f"[{target}] PACF ê³„ì‚° ì‹œ ìš”ì²­ëœ nlags({max_lag})ê°€ ìƒ˜í”Œ í¬ê¸°({n_samples_pacf})ì— ë¹„í•´ ë„ˆë¬´ í½ë‹ˆë‹¤. nlagsë¥¼ {max_permissible_nlags_pacf}ë¡œ ì¡°ì •í•©ë‹ˆë‹¤.", level="warning")
            actual_nlags_pacf = max_permissible_nlags_pacf
        elif max_permissible_nlags_pacf == 0:
            warnings.warn(
                f"Sample size {n_samples_pacf} is too small to compute PACF with requested max_lag {max_lag}. Setting nlags for PACF to 0.",
                UserWarning
            )
            log(f"[{target}] ìƒ˜í”Œ í¬ê¸°({n_samples_pacf})ê°€ ë„ˆë¬´ ì‘ì•„ PACFë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. PACF nlagsë¥¼ 0ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.", level="warning")
            actual_nlags_pacf = 0
        if actual_nlags_pacf > 0:
            try:
                pacf_values = pacf(target_data_dropna, nlags=actual_nlags_pacf, method='ywm')
            except ValueError as e:
                log(f"[{target}] PACF ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ (nlags={actual_nlags_pacf}, samples={n_samples_pacf}): {e}", level="error")
                pacf_values = np.array([])
        else:
            pacf_values = np.array([])

        significant_acf_lags = np.where(acf_values > 0.2)[0] if acf_values.size > 0 else []
        strong_acf_lags = np.where(acf_values > 0.6)[0] if acf_values.size > 0 else []
        significant_pacf_lags_indices = np.where(pacf_values > 0.2)[0] if pacf_values.size > 0 else []
        strong_pacf_lags_indices = np.where(pacf_values > 0.6)[0] if pacf_values.size > 0 else []

        detection_results[target]["significant_lags"] = list(strong_pacf_lags_indices)
        detection_results[target]["has_strong_periodicity"] = (len(strong_acf_lags) > 0 or len(strong_pacf_lags_indices) > 0)
        detection_results[target]["lag_needed"] = len(significant_pacf_lags_indices) > 0

        # 8. ì£¼ê¸°ì„± ë° ë‹¬ë ¥ ê¸°ë°˜ íŒ¨í„´
        try:
            detection_results[target]["has_periodicity"] = raw_data.index.freq is not None
            detection_results[target]["has_daily_pattern"] = raw_data.index.hour.value_counts(normalize=True).max() > 0.2
            detection_results[target]["has_weekly_pattern"] = raw_data.index.dayofweek.value_counts(normalize=True).max() > 0.2
            detection_results[target]["has_monthly_pattern"] = raw_data.index.day.value_counts(normalize=True).max() > 0.1
            detection_results[target]["has_quarterly_pattern"] = raw_data.index.month.isin([3, 6, 9, 12]).mean() > 0.2

            raw_data['day_of_week'] = raw_data.index.dayofweek
            raw_data['is_weekend'] = (raw_data['day_of_week'] >= 5).astype(int)
            detection_results[target]["has_rest_day_effect"] = raw_data["is_weekend"].mean() > 0.1

            kr_holidays = holidays.KR()
            raw_data['is_holiday'] = raw_data.index.to_series().apply(lambda x: 1 if x in kr_holidays else 0)
            detection_results[target]["has_holiday_effect"] = raw_data["is_holiday"].mean() > 0.1
            detection_results[target]["has_rest_day_pattern"] = (
                raw_data["is_holiday"].mean() > 0.10 and raw_data["is_weekend"].mean() > 0.10
            )
            detection_results[target]["has_holiday_shift"] = (
                raw_data["is_weekend"].shift(1).corr(raw_data[target]) > 0.2 or
                raw_data["is_holiday"].shift(1).corr(raw_data[target]) > 0.2
            )
        except Exception as err:
            detection_results[target]["has_periodicity"] = False
            detection_results[target]["has_daily_pattern"] = False
            detection_results[target]["has_weekly_pattern"] = False
            detection_results[target]["has_monthly_pattern"] = False
            detection_results[target]["has_quarterly_pattern"] = False
            detection_results[target]["has_rest_day_effect"] = False
            detection_results[target]["has_holiday_effect"] = False
            detection_results[target]["has_rest_day_pattern"] = False
            detection_results[target]["has_holiday_shift"] = False
            log(f"{target} ì»¬ëŸ¼ì—ì„œ ì£¼ê¸°ì„±/ë‹¬ë ¥ íŒ¨í„´ íƒì§€ ì¤‘ ì˜¤ë¥˜: {err}", level="warning")

        # 9. ì§‘ê³„ ìœ í˜• Feature ì¶”ì²œ
        try:
            series_hourly = raw_data[target].resample('1h').agg(['mean', 'median', 'min', 'max', 'std', 'sum']).dropna()
            std_dev = series_hourly.std()
            iqr_value = np.percentile(series_hourly, 75) - np.percentile(series_hourly, 25)
            skewness = series_hourly.apply(skew)
            kurt = series_hourly.apply(kurtosis)
            q1 = np.percentile(series_hourly, 25)
            q3 = np.percentile(series_hourly, 75)
            outlier_ratio = ((series_hourly < (q1 - 1.5 * iqr_value)) | (series_hourly > (q3 + 1.5 * iqr_value))).mean()
            rolling_mean_diff = series_hourly.rolling(window=5, min_periods=1).mean().diff().abs().mean()
            acf_values_hourly = acf(series_hourly['mean'], nlags=24)
            pacf_values_hourly = pacf(series_hourly['mean'], nlags=24)
            hourly_pattern = np.where(acf_values_hourly > 0.5)[0]
            daily_pattern = np.where(acf_values_hourly > 0.7)[0]
            recommended_lags = sorted([lag for lag in set(hourly_pattern) | set(daily_pattern) if lag > 0])

            recommended_aggregations = []
            if outlier_ratio.mean() > 0.1:
                recommended_aggregations.append("median")
            if rolling_mean_diff.mean() > std_dev.mean() * 0.5:
                recommended_aggregations.append("rolling_mean")
            if len(recommended_aggregations) == 0:
                recommended_aggregations.append("mean")

            detection_results[target]["recommended_aggregations"] = recommended_aggregations
            detection_results[target]["recommended_lags"] = recommended_lags
            detection_results[target]["strong_pacf_lags"] = list(np.where(pacf_values_hourly > 0.6)[0])
            detection_results[target]["significant_pacf_lags"] = list(np.where(pacf_values_hourly > 0.2)[0])
            detection_results[target]["std_dev"] = std_dev.to_dict()
            detection_results[target]["skewness"] = skewness.to_dict()
            detection_results[target]["kurtosis"] = kurt.to_dict()
            detection_results[target]["outlier_ratio"] = outlier_ratio.to_dict()
            detection_results[target]["rolling_mean_diff"] = rolling_mean_diff.to_dict()
        except Exception as e:
            detection_results[target]["error"] = str(e)
            detection_results[target]["recommended_aggregations"] = ["unknown"]

    return detection_results


def handle_missing_values(df, method="interpolate", fill_value=None, threshold=0.1):
    """
    ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜
    :param df: ì²˜ë¦¬í•  ë°ì´í„°í”„ë ˆì„
    :param method: ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë°©ë²• ('drop', 'ffill', 'bfill', 'mean', 'median', 'mode', 'fill', 'interpolate', 'polynomial', 'spline')
    :param fill_value: íŠ¹ì • ê°’ìœ¼ë¡œ ì±„ìš¸ ê²½ìš° ì‚¬ìš©
    :param threshold: ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ íŠ¹ì • ê°’ ì´ìƒì¸ ì—´ ì œê±° ê¸°ì¤€
    :return: ê²°ì¸¡ì¹˜ê°€ ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
    """
    missing_ratio = df.isnull().mean()
    
    # ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ íŠ¹ì • threshold ì´ìƒì¸ ì—´ ì‚­ì œ
    high_missing_cols = missing_ratio[missing_ratio > threshold].index
    if len(high_missing_cols) > 0:
        print(f"âš ï¸ Warning: High missing ratio detected in columns: {list(high_missing_cols)} â†’ Dropped")
        df = df.drop(columns=high_missing_cols)

    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë°©ë²• ì ìš©
    if method == "drop":
        df = df.dropna()
    elif method == "ffill":
        df = df.ffill()
    elif method == "bfill":
        df = df.bfill()
    elif method == "mean":
        df = df.fillna(df.mean())
    elif method == "median":
        df = df.fillna(df.median())
    elif method == "mode":
        df = df.fillna(df.mode().iloc[0])
    elif method == "fill":
        df = df.fillna(fill_value)
    elif method == "interpolate":
        df = df.interpolate(method='linear', limit_direction='both')
    elif method == "polynomial":
        df = df.interpolate(method='polynomial', order=2, limit_direction='both')
    elif method == "spline":
        df = df.interpolate(method='spline', order=2, limit_direction='both')
    
    return df

def generate_lag_features(df_no_outliers, target_columns, lag_results, target_df=None):
    """
    íƒì§€ëœ Lag ì •ë³´ë¥¼ ë°˜ì˜í•˜ì—¬ Moderate ë° Strong ìˆ˜ì¤€ì˜ Lag ë³€ìˆ˜ë§Œ ìƒì„±
    
    :param df_no_outliers: ì´ìƒì¹˜ ì œê±°ëœ DataFrame
    :param target_columns: ì˜ˆì¸¡ ëŒ€ìƒì´ ë˜ëŠ” ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
    :param lag_results: íƒì§€ëœ Lag ê²°ê³¼ (ìƒê´€ê´€ê³„ ê°•ë„ í¬í•¨)
    :param target_df: ë¯¸ë˜ ì˜ˆì¸¡ì„ ìœ„í•œ ë³„ë„ì˜ ëŒ€ìƒ DataFrame (ê¸°ë³¸ê°’: None)
    :return: (Lag ë³€ìˆ˜ê°€ ì¶”ê°€ëœ DataFrame, ì¶”ê°€ëœ lag ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸)
    """
    added_lag_columns = []
    for target in target_columns:
        if not target in lag_results:
            ensembleForecaster_logger.debug(f"íƒì§€ëœ {target} Lag ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ë¬´ì‹œ")
            continue  # íƒì§€ëœ Lag ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ë¬´ì‹œ
    
        selected_lags = lag_results[target]["recommended_lags"]
        if not selected_lags:
            ensembleForecaster_logger.debug("ìœ ì˜ë¯¸í•œ {target} Lagì´ ì—†ìœ¼ë©´ ê±´ë„ˆëœ€")
            continue  # ìœ ì˜ë¯¸í•œ Lagì´ ì—†ìœ¼ë©´ ê±´ë„ˆëœ€
        if target_df is not None:
            # target_df ê¸°ë°˜ìœ¼ë¡œ lag ë³€ìˆ˜ ìƒì„±
            future_data = df_no_outliers.copy()
            for lag in selected_lags:
                lag_column = f"{target}_lag_{lag}"
                future_data[lag_column] = target_df[target].shift(int(lag))
                added_lag_columns.append(lag_column)
            future_data.dropna(inplace=True)
            return future_data, added_lag_columns  # target_df ê¸°ë°˜ ìƒì„± ì‹œ ë°˜í™˜

        # ê¸°ì¡´ df_no_outliersì—ì„œ lag ë³€ìˆ˜ ìƒì„±
        for lag in selected_lags:
            lag_column = f"{target}_lag_{lag}"
            df_no_outliers[lag_column] = df_no_outliers[target].shift(int(lag))
            added_lag_columns.append(lag_column)

    df_no_outliers.dropna(inplace=True)  # ê²°ì¸¡ì¹˜ ì œê±°
    # âœ… 7. NaNë§Œ í¬í•¨ëœ ì»¬ëŸ¼ í™•ì¸ í›„ ì œê±°
    nan_columns = df_no_outliers.columns[df_no_outliers.isna().all()].tolist()
    if nan_columns:
        ensembleForecaster_logger.info(f"ğŸš¨ NaNë§Œ í¬í•¨ëœ ì»¬ëŸ¼ ì œê±°: {nan_columns}")
        df_no_outliers = df_no_outliers.drop(columns=nan_columns)
        added_lag_columns = [col for col in added_lag_columns if col not in nan_columns]  # ì¶”ê°€ëœ ì»¬ëŸ¼ì—ì„œ ì œê±°
        
    ensembleForecaster_logger.info(f"[{target}] ì ìš©ëœ lag ë³€ìˆ˜: {added_lag_columns}")
    
    return df_no_outliers, added_lag_columns

def optimize_outlier_removal(raw_data, target_columns=None, max_iterations=2, noise_threshold=0.02, 
                             threshold=0.05, method="iqr", max_removal_ratio=0.05, winsorize_limits=[0.05, 0.05]):
    """
    ì´ìƒì¹˜ ì œê±°ë¥¼ ìµœì í™”í•˜ëŠ” í•¨ìˆ˜.
    - ì—¬ëŸ¬ ê°œì˜ `value` ì»¬ëŸ¼ì„ ì§€ì›
    - ì œê±°ëœ ë°ì´í„° ë¹„ìœ¨ì´ ì„¤ì •í•œ ê°’(ê¸°ë³¸ 5%)ì„ ì´ˆê³¼í•˜ë©´ ê²½ê³  ë°œìƒ
    - LOF(Local Outlier Factor) ì¤‘ë³µ ê°’ ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ `n_neighbors=30` ì„¤ì •
    
    Parameters:
        raw_data (pd.DataFrame): ì´ìƒì¹˜ ì œê±°í•  ë°ì´í„°í”„ë ˆì„
        target_columns (list): ì´ìƒì¹˜ë¥¼ ì œê±°í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸: ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼)
        max_iterations (int): ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
        noise_threshold (float): ë…¸ì´ì¦ˆ ë¹„ìœ¨ì´ ì´ ê°’ ì´í•˜ê°€ ë˜ë©´ ì¤‘ì§€
        threshold (float): ì´ìƒì¹˜ ì œê±° ë¹„ìœ¨ì´ ì´ ê°’ ì´ìƒì´ë©´ ê²½ë³´ ë°œìƒ (ê¸°ë³¸ê°’ 5%)
        method (str): ì´ìƒì¹˜ ì œê±° ë°©ë²• ("rolling_mean", "iqr", "zscore", "winsorizing", "lof", "dbscan")
        max_removal_ratio (float): í•œ ë²ˆì— ì œê±° ê°€ëŠ¥í•œ ìµœëŒ€ ë¹„ìœ¨ (ê¸°ë³¸ê°’ 5%)
        winsorize_limits (list): Winsorizing ì ìš© ë²”ìœ„ (ê¸°ë³¸ê°’ [0.05, 0.05])

    Returns:
        pd.DataFrame: ì´ìƒì¹˜ê°€ ìµœì  ì œê±°ëœ ë°ì´í„°í”„ë ˆì„
    """
    
    if target_columns is None:
        target_columns = raw_data.select_dtypes(include=[np.number]).columns.tolist()

    initial_count = len(raw_data)  # ì´ˆê¸° ë°ì´í„° ê°œìˆ˜
    initial_raw_data = raw_data.copy()  # ì›ë³¸ ë°ì´í„° ë³µì‚¬

    iteration = 0
    while iteration < max_iterations:
        iteration += 1
        removed_rows = 0

        for col in target_columns:
            before_removal_count = len(raw_data)

            if method == "iqr":
                # IQR ê¸°ë°˜ ì œê±° (ë²”ìœ„ë¥¼ ë” ì™„í™”)
                Q1 = raw_data[col].quantile(0.10)  # ê¸°ì¡´ 0.25 â†’ ì™„í™”
                Q3 = raw_data[col].quantile(0.90)  # ê¸°ì¡´ 0.75 â†’ ì™„í™”
                IQR = Q3 - Q1
                mask = (raw_data[col] >= (Q1 - 1.0 * IQR)) & (raw_data[col] <= (Q3 + 1.0 * IQR))

            elif method == "zscore":
                # Z-score ê¸°ë°˜ ì œê±° (ì„ê³„ê°’ ì¡°ì •)
                z_scores = zscore(raw_data[col].dropna())
                mask = np.abs(z_scores) <= 2.5  # ê¸°ì¡´ 3 â†’ ì™„í™”

            elif method == "rolling_mean":
                # Rolling Mean ê¸°ë°˜ ì œê±°
                rolling_mean = raw_data[col].rolling(window=7, min_periods=1).mean()  # ê¸°ì¡´ 5 â†’ 7
                std_dev = rolling_mean.std()
                mask = np.abs(raw_data[col] - rolling_mean) <= std_dev * 1.5  # ê¸°ì¡´ 1.0 â†’ ì™„í™”

            elif method == "winsorizing":
                # Winsorizing (ê·¹ë‹¨ê°’ ëŒ€ì²´)
                raw_data[col] = winsorize(raw_data[col], limits=winsorize_limits)
                continue  # ë°ì´í„° ë³€í™˜ì´ë¯€ë¡œ ì‚­ì œ í•„ìš” ì—†ìŒ

            elif method == "lof":
                # LOF ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€ (n_neighbors ì¦ê°€)
                lof = LocalOutlierFactor(n_neighbors=30)
                lof_labels = lof.fit_predict(raw_data[[col]])
                mask = lof_labels != -1  # LOFì—ì„œ -1ì€ ì´ìƒì¹˜

            elif method == "dbscan":
                # DBSCAN ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€
                db = DBSCAN(eps=0.8, min_samples=10).fit(raw_data[[col]])  # ê¸°ì¡´ eps=0.5, min_samples=5 â†’ ì™„í™”
                mask = db.labels_ != -1  # DBSCANì—ì„œ -1ì€ ì´ìƒì¹˜

            else:
                raise ValueError("ì§€ì›ë˜ì§€ ì•ŠëŠ” methodì…ë‹ˆë‹¤. ('iqr', 'zscore', 'rolling_mean', 'winsorizing', 'lof', 'dbscan' ì¤‘ ì„ íƒ)")


            # ì´ìƒì¹˜ ì œê±° ì ìš©
            raw_data = raw_data[mask]
            removed_rows += before_removal_count - len(raw_data)

        # ì´ìƒì¹˜ ì œê±° ë¹„ìœ¨ ê³„ì‚°
        removed_ratio = removed_rows / initial_count

        # í•œ ë²ˆì— ë„ˆë¬´ ë§ì€ ë°ì´í„°ê°€ ì‚­ì œë˜ë©´ ê²½ê³  ë° ì›ë³¸ ë³µêµ¬
        if removed_ratio > max_removal_ratio:
            ensembleForecaster_logger.warning(f"âš ï¸ {method} ì´ìƒì¹˜ ì œê±° ë¹„ìœ¨ì´ {removed_ratio:.2%}ë¡œ {max_removal_ratio:.2%} ì´ˆê³¼! ì›ë³¸ ë°ì´í„°ë¡œ ë³µêµ¬í•©ë‹ˆë‹¤. âš ï¸")
            return raw_data.copy() if removed_rows == 0 else initial_raw_data.copy()

        if removed_ratio < noise_threshold:
            ensembleForecaster_logger.info(f"{method} ë…¸ì´ì¦ˆ ë¹„ìœ¨ {removed_ratio:.2%} ì´í•˜ë¡œ ê°ì†Œ, ì´ìƒì¹˜ ì œê±° ìµœì í™” ì™„ë£Œ")
            break
    ensembleForecaster_logger.info(f"{method} ì´ìƒì¹˜ ìµœì í™” ì™„ë£Œ âœ…")
    return raw_data


def generate_autocorrelation_features(raw_data, target_columns, detect_results):
    """
    íƒì§€ëœ íŒ¨í„´ì„ ê¸°ë°˜ìœ¼ë¡œ Sin/Cos ë³€í™˜ì„ ìë™ ì ìš©í•˜ëŠ” í•¨ìˆ˜.

    Parameters:
        raw_data (pd.DataFrame): ì›ë³¸ ë°ì´í„°í”„ë ˆì„ (datetime index í•„ìš”)
        detect_results (dict): íƒì§€ëœ íŒ¨í„´ ê²°ê³¼

    Returns:
        pd.DataFrame: ë³€í™˜ëœ ë°ì´í„°í”„ë ˆì„
        list: ì¶”ê°€ëœ íŠ¹ì§• ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
    """
    added_columns = []
    additional_features = {}  # ì—¬ëŸ¬ ì»¬ëŸ¼ì„ ì„ì‹œë¡œ ë‹´ì•„ë‘ 
    
    # ê³µí†µ holiday/weekend feature, í•œ ë²ˆë§Œ ê³„ì‚° (ëª¨ë“  íƒ€ê²Ÿ ê³µí†µì ìš© ì‹œ)
    idx = raw_data.index
    kr_holidays = holidays.KR()
    is_holiday = idx.to_series().apply(lambda x: 1 if x in kr_holidays else 0)
    day_of_week = idx.dayofweek
    is_weekend = (day_of_week >= 5).astype(int)
    
    # 1. í•˜ë£¨ ë‹¨ìœ„
    if all(detect_results[target].get("has_daily_pattern", False) for target in target_columns):
        additional_features["hour"] = idx.hour
        additional_features["minute"] = idx.minute
        additional_features["time_of_day"] = idx.hour * 60 + idx.minute
        added_columns += ["hour", "minute", "time_of_day"]
        
    # 2. ì£¼ ë‹¨ìœ„
    if all(detect_results[target].get("has_weekly_pattern", False) for target in target_columns):
        additional_features["day_of_week"] = day_of_week
        additional_features["is_weekend"] = is_weekend
        added_columns += ["day_of_week", "is_weekend"]

    # 3. ì›” ë‹¨ìœ„
    if all(detect_results[target].get("has_monthly_pattern", False) for target in target_columns):
        additional_features["month"] = idx.month
        additional_features["is_month_start"] = idx.is_month_start.astype(int)
        additional_features["is_month_end"] = idx.is_month_end.astype(int)
        additional_features["day_of_month"] = idx.day
        added_columns += ["month", "is_month_start", "is_month_end", "day_of_month"]
        
    # 4. ë¶„ê¸° ë‹¨ìœ„
    if all(detect_results[target].get("has_quarterly_pattern", False) for target in target_columns):
        additional_features["quarter"] = idx.quarter
        added_columns += ["quarter"]
    
    for target in target_columns:
        # 1. í•˜ë£¨ ë‹¨ìœ„ íŒ¨í„´ì´ ìˆëŠ” ê²½ìš° â†’ í•˜ë£¨ ë‹¨ìœ„ sin/cos ë³€í™˜ ì¶”ê°€
        if detect_results[target].get("has_daily_pattern", False):
            additional_features[f"{target}_hour_sin"] = np.sin(2 * np.pi * idx.hour / 24)
            additional_features[f"{target}_hour_cos"] = np.cos(2 * np.pi * idx.hour / 24)
            added_columns += [f"{target}_hour_sin", f"{target}_hour_cos"]

        # 2. ì£¼ ë‹¨ìœ„ íŒ¨í„´ì´ ìˆëŠ” ê²½ìš° â†’ ì£¼ ë‹¨ìœ„ sin/cos ë³€í™˜ ì¶”ê°€
        if detect_results[target].get("has_weekly_pattern", False):
            additional_features[f"{target}_week_sin"] = np.sin(2 * np.pi * day_of_week / 7)
            additional_features[f"{target}_week_cos"] = np.cos(2 * np.pi * day_of_week / 7)
            added_columns += [f"{target}_week_sin", f"{target}_week_cos"]

        # 3. ì›” ë‹¨ìœ„ íŒ¨í„´ì´ ìˆëŠ” ê²½ìš° â†’ ì›” ë‹¨ìœ„ sin/cos ë³€í™˜ ì¶”ê°€
        if detect_results[target].get("has_monthly_pattern", False):
            additional_features[f"{target}_month_sin"] = np.sin(2 * np.pi * idx.month / 12)
            additional_features[f"{target}_month_cos"] = np.cos(2 * np.pi * idx.month / 12)
            added_columns += [f"{target}_month_sin", f"{target}_month_cos"]

        # 4. ë¶„ê¸° ë‹¨ìœ„ íŒ¨í„´ì´ ìˆëŠ” ê²½ìš° â†’ ë¶„ê¸° ë‹¨ìœ„ sin/cos ë³€í™˜ ì¶”ê°€
        if detect_results[target].get("has_quarterly_pattern", False):
            additional_features[f"{target}_quarter_sin"] = np.sin(2 * np.pi * idx.quarter / 4)
            additional_features[f"{target}_quarter_cos"] = np.cos(2 * np.pi * idx.quarter / 4)
            added_columns += [f"{target}_quarter_sin", f"{target}_quarter_cos"]

        # âœ… ê³µíœ´ì¼ ë° íœ´ì¼ ê´€ë ¨ íŠ¹ì§• ì¶”ê°€
        if detect_results[target].get("has_quarterly_pattern", False):
            additional_features[f"{target}_is_rest_day_holiday"] = ((is_weekend == 1) | (is_holiday == 1)).astype(int)
            added_columns += [f"{target}_is_rest_day_holiday"]
        else:
            if detect_results[target].get("has_rest_day_effect", False):
                additional_features[f"{target}_is_weekend"] = is_weekend
                added_columns += [f"{target}_is_weekend"]
            if detect_results[target].get("has_holiday_effect", False):
                additional_features[f"{target}_is_holiday"] = is_holiday
                added_columns += [f"{target}_is_holiday"]
        # ê³µíœ´ì¼ ì´ë™íš¨ê³¼
        if detect_results[target].get("has_holiday_shift", False):
            additional_features[f"{target}_is_holiday_prev"] = is_holiday.shift(1, fill_value=0)
            additional_features[f"{target}_is_holiday_next"] = is_holiday.shift(-1, fill_value=0)
            added_columns += [f"{target}_is_holiday_prev", f"{target}_is_holiday_next"]
        # raw_data.drop(columns=[\'_is_weekend\'], inplace=True) # ì‚­ì œ
        # raw_data.drop(columns=[\'_is_holiday\'], inplace=True) # ì‚­ì œ
        # raw_data.drop(columns=[\'_day_of_week\'], inplace=True) # ì‚­ì œ

    # í•œ ë²ˆì— ì¶”ê°€
    new_features = pd.DataFrame(additional_features, index=raw_data.index)
    result_data = pd.concat([raw_data, new_features], axis=1)

    # âœ… 7. NaNë§Œ í¬í•¨ëœ ì»¬ëŸ¼ í™•ì¸ í›„ ì œê±°
    nan_columns = result_data.columns[result_data.isna().all()].tolist()
    if nan_columns:
        ensembleForecaster_logger.info(f"ğŸš¨ NaNë§Œ í¬í•¨ëœ ì»¬ëŸ¼ ì œê±°: {nan_columns}")
        result_data = result_data.drop(columns=nan_columns)
        added_columns = [col for col in added_columns if col not in nan_columns]
        
    ensembleForecaster_logger.info(f"[{target}] ì ìš©ëœ ì£¼ê¸°ì  ë³€í™˜: {added_columns}")
    
    # defragmentation
    result_data = result_data.copy()
    
    return result_data, added_columns # raw_data ëŒ€ì‹  result_data ë°˜í™˜

def generate_aggregation_features(raw_data, target_columns, detect_results):
    """
    ì§‘ê³„ ìœ í˜• ì¶”ì²œ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ì§‘ê³„ ë³€ìˆ˜ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.

    Parameters:
        raw_data (pd.DataFrame): ì›ë³¸ ë°ì´í„°í”„ë ˆì„ (datetime index í•„ìš”)
        target_columns (list): ì§‘ê³„í•  ëŒ€ìƒ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        detect_results (dict): íƒì§€ëœ íŒ¨í„´ ê²°ê³¼

    Returns:
        pd.DataFrame: ë³€í™˜ëœ ë°ì´í„°í”„ë ˆì„
        list: ì¶”ê°€ëœ íŠ¹ì§• ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
    """

    added_columns = []
    df = raw_data.copy() # ì›ë³¸ ë°ì´í„°í”„ë ˆì„ ë³µì‚¬í•˜ì—¬ ì‚¬ìš©
    additional_agg_features = {} # ìƒˆë¡œìš´ íŠ¹ì„±ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬

    for target in target_columns:
        recommended_aggregations = detect_results[target].get("recommended_aggregations", []) # í‚¤ ì¡´ì¬ í™•ì¸
        default_methods = ['min', 'max', 'std', 'sum']

        # âœ… ì¶”ì²œëœ ì§‘ê³„ ìœ í˜• ì¶”ê°€
        for method in recommended_aggregations:
            if method not in default_methods:
                default_methods.append(method)

        # âœ… 2. ì›ë³¸ ë°ì´í„° ë³´ê°„ (`resample('1h')` ì „ì— NaN ë°©ì§€)
        # df[target] = df[target].ffill() # ì´ ë¶€ë¶„ì€ resample ì „ì— ì›ë³¸ seriesì— ëŒ€í•´ ìˆ˜í–‰
        
        # âœ… 3. 1ì‹œê°„ ë‹¨ìœ„ ì§‘ê³„ ìˆ˜í–‰
        valid_agg_methods = ['min', 'max', 'std', 'sum', 'mean', 'median', 'first', 'last', 'count', 'nunique', 'ohlc']
        
        resample_methods = {
            method: method for method in default_methods 
            if method != "diff_mean" and method in valid_agg_methods
        }

        try:
            # ëŒ€ìƒ ì»¬ëŸ¼ì´ dfì— ìˆëŠ”ì§€ í™•ì¸
            if target not in df.columns:
                ensembleForecaster_logger.warning(f"[{target}] ì»¬ëŸ¼ì´ ë°ì´í„°í”„ë ˆì„ì— ì—†ì–´ ì§‘ê³„ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                continue

            target_series_for_resample = df[target].ffill() # ë¦¬ìƒ˜í”Œë§ ì „ ffill

            if not resample_methods:
                ensembleForecaster_logger.warning(f"[{target}] ìœ íš¨í•œ ì§‘ê³„ ë©”ì†Œë“œê°€ ì—†ì–´ ì§‘ê³„ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                aggregated_df = pd.DataFrame(index=df.index) # ì¸ë±ìŠ¤ ë§ì¶°ì£¼ê¸°
            else:
                aggregated_df = target_series_for_resample.resample('1h').agg(resample_methods).ffill()

            # âœ… 4. diff_mean ì¶”ê°€
            if "diff_mean" in recommended_aggregations and 'mean' in aggregated_df.columns:
                diff_mean_col_name = f"{target}_diff_mean"
                diff_series = aggregated_df['mean'].diff()
                interpolated_diff = diff_series.interpolate(method="linear")
                additional_agg_features[diff_mean_col_name] = interpolated_diff.fillna(0)
                added_columns.append(diff_mean_col_name)

            if 'max' in aggregated_df.columns and 'min' in aggregated_df.columns:
                # âœ… 5. max-min ì°¨ì´ê°’ ì¶”ê°€
                range_col_name = f"{target}_range"
                range_series = aggregated_df["max"] - aggregated_df["min"]
                interpolated_range = range_series.interpolate(method="linear")
                additional_agg_features[range_col_name] = interpolated_range.fillna(0)
                added_columns.append(range_col_name)
            
            # Lag ê¸°ëŠ¥ ì œê±°ë¨

        except Exception as e:
            ensembleForecaster_logger.error(f"[{target}] ì§‘ê³„ íŠ¹ì„± ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True) # ìƒì„¸ ì˜¤ë¥˜ ë¡œê¹…
            continue
    
    # ëª¨ë“  ë£¨í”„ê°€ ëë‚œ í›„, additional_agg_featuresì— ìˆëŠ” ëª¨ë“  ìƒˆ íŠ¹ì„±ì„ dfì— í•œ ë²ˆì— ì¶”ê°€
    if additional_agg_features:
        new_features_df = pd.DataFrame(additional_agg_features, index=df.index)
        df = pd.concat([df, new_features_df], axis=1)

    # âœ… 7. NaNë§Œ í¬í•¨ëœ ì»¬ëŸ¼ í™•ì¸ í›„ ì œê±° (df ê¸°ì¤€)
    if not df.empty: # dfê°€ ë¹„ì–´ìˆì§€ ì•Šì„ ë•Œë§Œ ì‹¤í–‰
        nan_columns = df.columns[df.isna().all()].tolist()
        if nan_columns:
            ensembleForecaster_logger.info(f"ğŸš¨ NaNë§Œ í¬í•¨ëœ ì»¬ëŸ¼ ì œê±°: {nan_columns}")
            df = df.drop(columns=nan_columns)
            added_columns = [col for col in added_columns if col not in nan_columns]
    
    # ensembleForecaster_logger.info(f"ì ìš©ëœ ì§‘ê³„ ìœ í˜• ì¶”ì²œ: {added_columns}") # target ì •ë³´ê°€ ì—†ì–´ ë£¨í”„ ë°–ì—ì„œëŠ” ë¶€ì ì ˆ
    
    return df, added_columns # ìˆ˜ì •ëœ dfì™€ ì¶”ê°€ëœ ì»¬ëŸ¼ëª… ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

def detect_noise_level(raw_data, target_columns):
    """
    ë°ì´í„° ë‚´ ì´ìƒì¹˜(ë…¸ì´ì¦ˆ) ë¹„ìœ¨ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜.
    
    Parameters:
        raw_data (pd.DataFrame): íƒ€ê²Ÿ ì»¬ëŸ¼ì´ ìˆëŠ” ë°ì´í„°í”„ë ˆì„
        target_columns (list): ì´ìƒì¹˜ë¥¼ ë¶„ì„í•  ëŒ€ìƒ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        float: ë…¸ì´ì¦ˆ ë¹„ìœ¨ (0~1 ì‚¬ì´ ê°’)
    """
    total_outliers = pd.Series(False, index=raw_data.index)

    for col in target_columns:
        # IQR ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€
        Q1 = raw_data[col].quantile(0.25)
        Q3 = raw_data[col].quantile(0.75)
        IQR = Q3 - Q1
        iqr_outliers = (raw_data[col] < (Q1 - 1.5 * IQR)) | (raw_data[col] > (Q3 + 1.5 * IQR))

        # Z-score ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€
        z_scores = zscore(raw_data[col].dropna())
        z_outliers = np.abs(z_scores) > 3

        # ì´ë™ í‰ê·  ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€
        rolling_mean = raw_data[col].rolling(window=5, min_periods=1).mean()
        std_dev = rolling_mean.std()
        rolling_outliers = np.abs(raw_data[col] - rolling_mean) > std_dev

        # ì´ìƒì¹˜ ë¹„ìœ¨ ì—…ë°ì´íŠ¸
        total_outliers |= iqr_outliers | z_outliers | rolling_outliers

    noise_ratio = total_outliers.sum() / len(raw_data)
    ensembleForecaster_logger.info(f"ë°ì´í„° ë…¸ì´ì¦ˆ ë¹„ìœ¨: {noise_ratio:.2%}")

    return noise_ratio

def detect_best_scaler(data):
    """
    ë°ì´í„° íƒì§€ë¥¼ í†µí•´ ìµœì ì˜ Scalerë¥¼ ìë™ìœ¼ë¡œ ì„ íƒí•˜ëŠ” í•¨ìˆ˜.

    Parameters:
        data (pd.Series or np.array): ìŠ¤ì¼€ì¼ë§í•  ëŒ€ìƒ ë°ì´í„° (1D)

    Returns:
        str: ì¶”ì²œ Scaler ì´ë¦„
        bool: ë¡œê·¸ ë³€í™˜ í•„ìš” ì—¬ë¶€
    """
    data = np.array(data).reshape(-1, 1)  # ë°ì´í„° ì°¨ì› ì¡°ì •
    log_transform_needed = False  # ê¸°ë³¸ì ìœ¼ë¡œ ë¡œê·¸ ë³€í™˜ì€ False

    # 1ï¸âƒ£ ë°ì´í„°ì˜ ìµœì†Œê°’ì´ 0 ì´í•˜ì¸ì§€ í™•ì¸ (MAPE ìµœì í™” ì‹œ ì¤‘ìš”)
    if np.min(data) <= 0:
        log_transform_needed = True  # 0 ì´í•˜ ê°’ì´ ìˆìœ¼ë©´ ë¡œê·¸ ë³€í™˜ í•„ìš”

    # 2ï¸âƒ£ ë°ì´í„°ì˜ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ í™•ì¸ â†’ StandardScaler ì í•©ì„± íŒë‹¨
    mean_val, std_val = np.mean(data), np.std(data)
    if np.abs(mean_val) < 1 and 0.5 < std_val < 1.5:
        return "StandardScaler", log_transform_needed

    # 3ï¸âƒ£ ë°ì´í„°ì˜ ìµœì†Œ/ìµœëŒ€ ë²”ìœ„ í™•ì¸ â†’ MinMaxScaler ì í•©ì„± íŒë‹¨
    if data.min() >= 0 and data.max() < 100:
        return "MinMaxScaler", log_transform_needed

    # 4ï¸âƒ£ ì´ìƒì¹˜(Outliers) ë¶„ì„ â†’ RobustScaler í•„ìš” ì—¬ë¶€ íŒë‹¨
    q1, q3 = np.percentile(data, [25, 75])
    iqr = q3 - q1
    outlier_count = np.sum((data < (q1 - 1.5 * iqr)) | (data > (q3 + 1.5 * iqr)))
    if outlier_count > 0.05 * len(data):  # ì´ìƒì¹˜ ë¹„ìœ¨ì´ 5% ì´ìƒì´ë©´ RobustScaler ì¶”ì²œ
        return "RobustScaler", log_transform_needed

    # 5ï¸âƒ£ ë°ì´í„° ë¶„í¬ ë¶„ì„ â†’ ì •ê·œì„± íŒë‹¨ (Shapiro-Wilk Test, Skewness, Kurtosis)
    skewness = skew(data.flatten())  # ì™œë„ (Skewness)
    kurt = kurtosis(data.flatten())  # ì²¨ë„ (Kurtosis)

    if abs(skewness) > 1 or abs(kurt) > 3:
        # ë¹„ì •ê·œ ë¶„í¬ì´ê³  ì™œë„ê°€ í¬ë‹¤ë©´ QuantileTransformer or PowerTransformer ì¶”ì²œ
        if np.min(data) > 0:  # ì–‘ìˆ˜ ë°ì´í„°ë¼ë©´ PowerTransformer ì¶”ì²œ
            return "PowerTransformer", log_transform_needed
        else:
            return "QuantileTransformer", log_transform_needed

    # 6ï¸âƒ£ ì–‘ìˆ˜ & ëŒ€ì¹­ì  ë°ì´í„°ì¼ ê²½ìš° â†’ MaxAbsScaler ì¶”ì²œ
    if np.min(data) >= 0 and np.max(data) <= 1:
        return "MaxAbsScaler", log_transform_needed

    # ê¸°ë³¸ê°’ìœ¼ë¡œ StandardScaler ë°˜í™˜
    return "StandardScaler", log_transform_needed

def apply_scaling(df, target_columns, mode="per_feature", expanded_features=[]):
    """
    ê°œë³„ëª¨ë“œ(Per-Feature Mode) & ë‹¨ì¼ëª¨ë“œ(Global Mode)ì—ì„œ Scalerë¥¼ ìë™ìœ¼ë¡œ ì ìš©í•˜ëŠ” í•¨ìˆ˜.

    Parameters:
        df (pd.DataFrame): ì›ë³¸ ë°ì´í„°í”„ë ˆì„
        target_columns (list): ìŠ¤ì¼€ì¼ë§í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        mode (str): "per_feature" (ê° íŠ¹ì§•ë³„ ë‹¤ë¥¸ Scaler ì ìš©) ë˜ëŠ” "global" (ì „ì²´ ë™ì¼ Scaler ì ìš©)

    Returns:
        pd.DataFrame: ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„°í”„ë ˆì„
        dict: ì ìš©ëœ Scaler ì •ë³´
    """
    if df.empty:
        ensembleForecaster_logger.warning("Input DataFrame to apply_scaling is empty. Returning empty structures.")
        # Ensure the return signature matches: scaled_df, scaler_dict, log_needed_dict, expanded_features
        return df.copy(), {}, {}, {}
    
    scaled_df = df.copy()
    scaler_dict = {}  # ê° ì»¬ëŸ¼ì— ì ìš©ëœ Scaler ì €ì¥
    log_needed_dict = {}
    targets = {}
    if mode == "per_feature":
        # âœ… ê°œë³„ëª¨ë“œ: ê° target_column ë³„ë¡œ ìµœì  Scaler ì„ íƒ
        for target in target_columns:
            data_to_scale = df[target].dropna()
            if data_to_scale.empty:
                ensembleForecaster_logger.warning(f"[{target}] ìŠ¤ì¼€ì¼ë§í•  ë°ì´í„°ê°€ ì—†ì–´ StandardScalerë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤ (ëª¨ë“  ê°’ì´ NaN).")
                best_scaler_name = "StandardScaler"
                log_needed = False
            else:
                best_scaler_name, log_needed = detect_best_scaler(data_to_scale)  # ìµœì  Scaler íƒì§€

            log_needed_dict[target] = log_needed # ë¡œê·¸ ë³€í™˜ í•„ìš” ì—¬ë¶€ ì €ì¥

            # Scaler ê°ì²´ ìƒì„±
            if best_scaler_name == "StandardScaler":
                scaler = StandardScaler()
            elif best_scaler_name == "MinMaxScaler":
                scaler = MinMaxScaler()
            elif best_scaler_name == "RobustScaler":
                scaler = RobustScaler()
            elif best_scaler_name == "QuantileTransformer":
                scaler = QuantileTransformer(output_distribution="uniform")
            elif best_scaler_name == "PowerTransformer":
                scaler = PowerTransformer()
            else:
                scaler = StandardScaler()  # ê¸°ë³¸ê°’
                
            scaler_dict[target] = scaler  # Scaler ì €ì¥
            scaled_df[target] = scaler.fit_transform(df[[target]])
            log_needed_dict[target] = log_needed  # log í•„ìš”ì—¬ë¶€ ì €ì¥

    elif mode == "global":
        # âœ… ë‹¨ì¼ëª¨ë“œ: ì „ì²´ ë°ì´í„°ì— í•˜ë‚˜ì˜ Scalerë§Œ ì ìš©
        combined_data = np.concatenate([df[target].dropna().values.reshape(-1, 1) for target in target_columns], axis=0)
        best_scaler_name, log_needed = detect_best_scaler(combined_data)  # ì „ì²´ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ ìµœì  Scaler íƒì§€

        # Scaler ê°ì²´ ìƒì„±
        if best_scaler_name == "StandardScaler":
            scaler = StandardScaler()
        elif best_scaler_name == "MinMaxScaler":
            scaler = MinMaxScaler()
        elif best_scaler_name == "RobustScaler":
            scaler = RobustScaler()
        elif best_scaler_name == "QuantileTransformer":
            scaler = QuantileTransformer(output_distribution="uniform")
        elif best_scaler_name == "PowerTransformer":
            scaler = PowerTransformer()
        else:
            scaler = StandardScaler()  # ê¸°ë³¸ê°’

        # âœ… ëª¨ë“  target_columnsì— ë™ì¼í•œ Scaler ì ìš©
        for target in target_columns:
            scaled_df[target] = scaler.fit_transform(df[[target]])
            scaler_dict[target] = scaler  # ëª¨ë“  ì»¬ëŸ¼ì— ë™ì¼í•œ Scaler ì €ì¥
            log_needed_dict[target] = log_needed  # log í•„ìš”ì—¬ë¶€ ì €ì¥
            
    for col in target_columns:
        scaled_df[col] = scaled_df[col].fillna(df[col])
    scaled_df = scaled_df.loc[:, ~scaled_df.columns.duplicated()]
    expanded_features = scaled_df.columns.tolist()
    
    return scaled_df, scaler_dict, log_needed_dict, expanded_features

# === ì¾Œì ì§€ìˆ˜ í•¨ìˆ˜ ===
def discomfort_index(temp, hum):
    return 0.81 * temp + 0.01 * hum * (0.99 * temp - 14.3) + 46.3

def make_sensor_column_map(raw_data_columns, settings):
    # ì™¸ë¶€ ê³µê¸° ìƒíƒœ (AWS)
    aws_columns = {}
    for k, filters in zip(['TEMP', 'RH', 'CO2'], [settings.TEMP_FILTER, settings.HUMIDITY_FILTER, settings.CO2_FILTER]):
        for f in filters:
            candidates = [col for col in raw_data_columns if col.startswith(settings.OUT_DOOR) and f in col]
            if candidates:
                aws_columns[k] = candidates[0]  # ì²« ë²ˆì§¸ ë§¤ì¹­ ì»¬ëŸ¼ë§Œ ì‚¬ìš© (ì—¬ëŸ¬ê°œë©´ ê·œì¹™ í™•ì¥ í•„ìš”)

    # ì‹¤ë‚´ ê³µê°„ë³„ ì„¼ì„œ ì»¬ëŸ¼ ë§¤í•‘
    iaraw_columns = {}
    indoor_spaces = set([col.split('_')[0] for col in raw_data_columns if col.startswith(settings.IN_DOOR)])
    for space in indoor_spaces:
        iaraw_columns[space] = {}
        # TEMP
        temp_col = next((col for col in raw_data_columns if col.startswith(space) and any(f in col for f in settings.TEMP_FILTER)), None)
        if temp_col: iaraw_columns[space]['TEMP'] = temp_col
        # RH
        rh_col = next((col for col in raw_data_columns if col.startswith(space) and any(f in col for f in settings.HUMIDITY_FILTER)), None)
        if rh_col: iaraw_columns[space]['RH'] = rh_col
        # CO2
        co2_col = next((col for col in raw_data_columns if col.startswith(space) and any(f in col for f in settings.CO2_FILTER)), None)
        if co2_col: iaraw_columns[space]['CO2'] = co2_col

    return aws_columns, iaraw_columns

# === ì¾Œì ì§€ìˆ˜ ê¸°ë°˜ í”¼ì²˜ ìƒì„± í•¨ìˆ˜ ===
def add_discomfort_features(raw_data, aws_columns, iaraw_columns):
    """
    Parameters:
        raw_data (pd.DataFrame): ì „ì²´ ë°ì´í„°í”„ë ˆì„
        aws_columns (dict): ì™¸ê¸° ìƒíƒœ ì»¬ëŸ¼ ë§¤í•‘ (TEMP, RH, CO2)
        iaraw_columns (dict): ì‹¤ë‚´ ê³µê°„ë³„ ì»¬ëŸ¼ ë§¤í•‘ {'ê³µê°„ëª…': {'TEMP': col, 'RH': col, 'CO2': col}}

    Returns:
        raw_data (pd.DataFrame): ì¾Œì ì§€ìˆ˜ ë° í”¼ì²˜ ì¶”ê°€ëœ ë°ì´í„°
        added_columns (list): ìƒˆë¡œ ì¶”ê°€ëœ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
    """
    added_columns = []

    # ì™¸ê¸° ê¸°ì¤€
    raw_data['outdoor_di'] = discomfort_index(raw_data[aws_columns['TEMP']], raw_data[aws_columns['RH']])
    added_columns.append('outdoor_di')

    for space, cols in iaraw_columns.items():
        temp_col, hum_col, co2_col = cols['TEMP'], cols['RH'], cols['CO2']

        di_col = f'{space}_di'
        raw_data[di_col] = discomfort_index(raw_data[temp_col], raw_data[hum_col])
        added_columns.append(di_col)

        level_col = f'{space}_level'
        raw_data[level_col] = pd.cut(raw_data[di_col],
            bins=[0, 68, 75, 100],
            labels=['comfort', 'uncomfortable', 'very_uncomfortable']
        )
        dummies = pd.get_dummies(raw_data[level_col], prefix=f'discomfort_{space}')
        raw_data = pd.concat([raw_data, dummies], axis=1)
        added_columns.extend(dummies.columns.tolist())

        # CO2 ë° í™˜ê¸° ê´€ë ¨
        co2_diff_col = f'{space}_co2_diff'
        raw_data[co2_diff_col] = raw_data[co2_col] - raw_data[aws_columns['CO2']]
        raw_data[f'{space}_need_vent'] = (raw_data[co2_diff_col] > 100).astype(int)
        raw_data[f'{space}_vent_burden'] = raw_data[f'{space}_need_vent'] * abs(
            raw_data[temp_col] - raw_data[aws_columns['TEMP']]
        )
        added_columns.extend([co2_diff_col, f'{space}_need_vent', f'{space}_vent_burden'])

    return raw_data, added_columns
